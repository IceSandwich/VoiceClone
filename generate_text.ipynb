{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, typing, jieba, pypinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/cxn/BZNSYP/000001-010000.txt\"\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "\tlines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('#\\d+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got dataset 10000\n"
     ]
    }
   ],
   "source": [
    "dataset: typing.List[typing.Tuple[str, str]]  = []\n",
    "for i in range(0, len(lines), 2):\n",
    "\tsplitpos = lines[i].index('\\t')\n",
    "\ttext = lines[i][splitpos+1:].strip('\\n')\n",
    "\ttext = regex.sub(\"\", text)\n",
    "\n",
    "\tphome = lines[i + 1].strip('\\n').strip()\n",
    "\tdataset.append((text, phome))\n",
    "print(f\"Got dataset {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('卡尔普陪外孙玩滑梯。', 'ka2 er2 pu3 pei2 wai4 sun1 wan2 hua2 ti1'),\n",
       " ('假语村言别再拥抱我。', 'jia2 yu3 cun1 yan2 bie2 zai4 yong1 bao4 wo3'),\n",
       " ('宝马配挂跛骡鞍，貂蝉怨枕董翁榻。',\n",
       "  'bao2 ma3 pei4 gua4 bo3 luo2 an1 diao1 chan2 yuan4 zhen3 dong3 weng1 ta4'),\n",
       " ('邓小平与撒切尔会晤。', 'deng4 xiao3 ping2 yu3 sa4 qie4 er3 hui4 wu4'),\n",
       " ('老虎幼崽与宠物犬玩耍。', 'lao2 hu3 you4 zai3 yu2 chong3 wu4 quan3 wan2 shua3')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got token set 2069\n"
     ]
    }
   ],
   "source": [
    "token_filename = \"assets/dataset/tokens.txt\"\n",
    "token_set: typing.Set[str] = set()\n",
    "with open(token_filename, 'r', encoding='utf-8') as f:\n",
    "\tfor line in f.readlines():\n",
    "\t\ttoken = line.strip('\\n').strip().split(' ')[0]\n",
    "\t\ttoken_set.add(token)\n",
    "print(f\"Got token set {len(token_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got baker token set 1607\n"
     ]
    }
   ],
   "source": [
    "baker_token_set = set()\n",
    "for (text, phome) in dataset:\n",
    "\tfor token in phome.split(' '):\n",
    "\t\tbaker_token_set.add(token)\n",
    "print(f\"Got baker token set {len(baker_token_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got diff set 768\n"
     ]
    }
   ],
   "source": [
    "diff_set = token_set - baker_token_set\n",
    "print(f\"Got diff set {len(diff_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abandon tokens: {'chi5', 'jun5', 'hao5', 'wang5', 'pa5', 'cong5', 'nao5', 'luo5', 'xie5', 'guo5', 'mao5', 'qiu5', 'guang5', 'jingr3', 'tun5', 'sai5', 'renr4', 'yanr3', 'lo5', 'wu5', 'zhuan5', 'lei5', 'bang5', 'tianr1', 'qiang5', 'nar4', 'lian5', 'shang5', 'fen5', 'yangr4', 'nang5', 'gong5', 'lang5', 'jian5', 'mai5', 'tui5', 'nv5', 'yao5', 'hu5', 'dan5', 'sheng5', 'dianr2', 'fan5', 'lai5', 'chuan5', 'pai5', 'yanr2', 'lv5', 'tuo5', 'mei5', 'wen5', 'shu5', 'liar3', 'chao5', 'di5', 'du5', 'wei5', 'dianr3', 'han5', 'duor3', 'ter4', 'cha5', 'huan5', 'hou5', 'xiao5', 'mo5', 'lu5', 'peng5', 'dunr3', 'mou5', 'e5', 'fu5', 'tao5', 'shei2', 'nar3', 'ma5', 'nai5', 'duir1', 'qin5', 'zai5', 'bu5', 'lanr4', 'he5', 'rang5', 'zao5', 'huir3', 'chen5', 'lie5', 'ying5', 'pian5', 'huir4', 'zhi5', 'huang5', 'xin5', 'yi5', 'quan5', 'menr5', 'yuan5', 'tan5', 'chu5', 'weir4', 'wanr1', 'sao5', 'run5', 'tei1', 'dao5', 'duir4', 'zong5', 'la5', 'tong5', 'me5', 'li5', 'shi5', 'sou5', 'shao5', 'gou5', 'chui5', 'su5', 'pan5', 'tanr1', 'sha5', 'gan5', 'qiao5', 'zhunr3', 'ran5', 'ng1', 'shuan5', 'wa5', 'jing5', 'qian5', 'zuo5', 'zhou5', 'po5', 'lin5', 'qing5', 'xiong5', 'dong5', 'zher4', 'bao5', 'ba5', 'da5', 'yang5', 'gu5', 'bei5', 'tongr4', 'dir2', 'cui5', 'pir2', 'gunr4', 'ha5', 'de5', 'tuan5', 'bian5', 'niu5', 'ei5', 'ya5', 'long5', 'mingr2', 'kuai5', 'zaor3', 'fang5', 'jia5', 'ming5', 'shai5', 'xi5', 'shuai5', 'mi5', 'er5', 'bo5', 'xu5', 'shen5', 'zhuang5', 'zheng5', 'rou5', 'zhu5', 'tuor3', 'ling5', 'o5', 'qie5', 'dou5', 'suan5', 'zu5', 'tai5', 'ti5', 'xia5', 'lir3', 'ger3', 'her2', 'cir2', 'ping5', 'gui5', 'cao5', 'qu5', 'yingr3', 'jie5', 'kan5', 'niang5', 'lan5', 'huor3', 'hair2', 'jiao5', 'fanr4', 'shir4', 'gair4', 'ju5', 'yu5', 'fur4', 'nong5', 'jiang5', 'nen5', 'ye5', 'yo5', 'duo5', 'teng5', 'zi5', 'shou5', 'dai5', 'men5', 'tang5', 'you5', 'zhe5', 'ger2', 'hui5', 'yong5', 'mian5', 'wanr2', 'pi5', 'jiar1', 'yuanr4', 'diao5', 'jir1', 'fa5', 'chan5', 'hanr4', 'ger4', 'die5', 'hai5', 'chang5', 'yir5', 'chuang5', 'menr2', 'kuair4', 'si5', 'kuang5', 'tour2', 'ting5', 'ren5', 'se5', 'sa5', 'suo5', 'IY1', 'zhei4', 'hun5', 'huor2', 'tou5', 'cai5', 'liang5', 'gang5', 'zhur3', 'dir4', 'xing5', 'jinr4', 'genr1', 'P', 'huanr1', 'ne5', 'ai5', 'na5', 'huo5', 'ben5', 'lao5', 'shuo5', 'niur1', 'jiu5', 'nar2', 'gua5', 'qi5', 'zha5', 'tu5', 'heng5', 'liao5', 'wo5', 'cheng5', 'daor4', 'banr3', 'mu5', 'kou5', 'far2', 'yue5', 'a5', 'yir4', 'ji5', 'dir3', 'hua5', 'ge5', 'nan5', 'wan5', 'feng5', 'ta5', 'jin5', 'le5', 'lou5', 'chou5', 'qir4', 'bai5', 'liu5'}\n"
     ]
    }
   ],
   "source": [
    "maps: typing.Dict[str, typing.Set[int]] = { x: set() for x in token_set } # { token: [line number] }\n",
    "abandon_token: typing.Set[str] = set()\n",
    "for i, (text, phome) in enumerate(dataset):\n",
    "\tfor token in phome.split(' '):\n",
    "\t\tif token not in token_set:\n",
    "\t\t\tabandon_token.add(token)\n",
    "\t\telse:\n",
    "\t\t\tmaps[token].add(i)\n",
    "print(f\"Abandon tokens:\", abandon_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wo', 0), ('na', 0), ('tun4', 0), ('ce', 0), ('nun2', 0)]\n",
      "[('bu4', 1165), ('you3', 1180), ('zai4', 1428), ('yi4', 1653), ('shi4', 2512)]\n"
     ]
    }
   ],
   "source": [
    "freq = [ (key, len(value)) for key, value in maps.items() ]\n",
    "freq.sort(key=lambda x: x[1])\n",
    "print(freq[:5])\n",
    "print(freq[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "agresive_maps: typing.Dict[str, typing.List[int]] = {} # token: [line number]\n",
    "for (key, value) in maps.items():\n",
    "\tline_length: typing.List[typing.Tuple[int, int]] = [] # (line number, length)\n",
    "\tfor line_num in value:\n",
    "\t\tline_length.append((line_num, len(dataset[line_num][1].split(' '))))\n",
    "\tline_length.sort(key=lambda x: x[1], reverse=True)\n",
    "\tagresive_maps[key] = [ x[0] for x in line_length ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got outputs: 1403\n"
     ]
    }
   ],
   "source": [
    "n_select = 3\n",
    "select_set: typing.Set[int] = set()\n",
    "for (token, count) in freq:\n",
    "\tfor i in agresive_maps[token][:n_select]:\n",
    "\t\tselect_set.add(i)\n",
    "outputs = [ dataset[i] for i in select_set]\n",
    "print(f\"Got outputs: {len(outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('躺在急救担架上的男子双目紧闭，头发散发出一股烧焦的味道。',\n",
       "  'tang3 zai4 ji2 jiu4 dan1 jia4 shang4 de5 nan2 zi3 shuang1 mu4 jin3 bi4 tou2 fa4 san4 fa1 chu1 yi4 gu3 shao1 jiao1 de5 wei4 dao4'),\n",
       " ('工业园区是承接产业转移、加速产业集聚、培育产业集群的主要载体。',\n",
       "  'gong1 ye4 yuan2 qu1 shi4 cheng2 jie1 chan3 ye4 zhuan3 yi2 jia1 su4 chan3 ye4 ji2 ju4 pei2 yu4 chan3 ye4 ji2 qun2 de5 zhu3 yao4 zai4 ti3'),\n",
       " ('那一刻，我才真正的懂你，就像懂我现在的自己。',\n",
       "  'na4 yi2 ke4 wo3 cai2 zhen1 zheng4 de5 dong2 ni3 jiu4 xiang4 dong2 wo3 xian4 zai4 de5 zi4 ji3'),\n",
       " ('由于列车长时间停靠，车厢内的空气越来越“闷”。',\n",
       "  'you2 yu2 lie4 che1 zhang3 shi2 jian1 ting2 kao4 che1 xiang1 nei4 de5 kong1 qi4 yue4 lai2 yue4 men1'),\n",
       " ('但如果按车队规模，一嗨数千辆车的量级绝对算不上最大。',\n",
       "  'dan4 ru2 guo3 an4 che1 dui4 gui1 mo2 yi4 hai1 shu4 qian1 liang4 che1 de5 liang4 ji2 jue2 dui4 suan4 bu2 shang4 zui4 da4')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got mandarin: 729\n"
     ]
    }
   ],
   "source": [
    "mandarin_file = \"assets/text/mandarin.txt\"\n",
    "mandarin_dataset: typing.List[typing.Tuple[str, str]] = []\n",
    "with open(mandarin_file, 'r', encoding='utf-8') as f:\n",
    "\tfor i, line in enumerate(f.readlines()):\n",
    "\t\ttext = line.strip('\\n').strip()\n",
    "\t\tnormalized = text.replace('，',',').replace('。', '.').replace('？','?').replace('！','!').replace('、', ',').replace('”','\"').replace('“','\"')\n",
    "\t\tcuts = list(jieba.cut(normalized))\n",
    "\t\tphome = []\n",
    "\t\tfor cut in cuts:\n",
    "\t\t\tif cut == ',' or cut == '.' or cut == '?' or cut == '!' or cut == '\"':\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tpinyins = pypinyin.lazy_pinyin(cut, style=pypinyin.Style.TONE3, tone_sandhi=True)\n",
    "\t\t\tfor pinyin in pinyins:\n",
    "\t\t\t\tphome.append(pinyin)\n",
    "\t\tphome = ' '.join(phome)\n",
    "\t\tmandarin_dataset.append((text, phome))\n",
    "print(f\"Got mandarin: {len(mandarin_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('他在家里吃苹果，喝茶，打电话，唱歌。',\n",
       "  'ta1 zai4 jia1 li3 chi1 ping2 guo3 he1 cha2 da3 dian4 hua4 chang4 ge1'),\n",
       " ('我去商店买水果和书，看到小猫在玩。',\n",
       "  'wo3 qu4 shang1 dian4 mai3 shui3 guo3 he2 shu1 kan4 dao4 xiao3 mao1 zai4 wan2'),\n",
       " ('小明去北京吃炸酱面，他在公园里看见了许多种类的鸟，甚至还遇到了一只大猩猩。',\n",
       "  'xiao3 ming2 qu4 bei3 jing1 chi1 zha2 jiang4 mian4 ta1 zai4 gong1 yuan2 li3 kan4 jian4 le xu3 duo1 zhong3 lei4 de niao3 shen4 zhi4 hai2 yu4 dao4 le yi4 zhi1 da4 xing1 xing1'),\n",
       " ('小猫学会了跳舞，爬上了高高的树。',\n",
       "  'xiao3 mao1 xue2 hui4 le tiao4 wu3 pa2 shang4 le gao1 gao1 de shu4'),\n",
       " ('风筝在天空中飞得又高又快，真是太漂亮了。',\n",
       "  'feng1 zheng1 zai4 tian1 kong1 zhong1 fei1 de2 you4 gao1 you4 kuai4 zhen1 shi4 tai4 piao4 liang4 le')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mandarin_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abandon tokens: {'5G', '—', 'AI', 'GDPR'}\n"
     ]
    }
   ],
   "source": [
    "mandarin_maps: typing.Dict[str, typing.Set[int]] = { x: set() for x in token_set }\n",
    "mandarin_abandon_token: typing.Set[str] = set()\n",
    "for i, (text, phome) in enumerate(mandarin_dataset):\n",
    "\tfor token in phome.split(' '):\n",
    "\t\tif token not in token_set:\n",
    "\t\t\tmandarin_abandon_token.add(token)\n",
    "\t\telse:\n",
    "\t\t\tmandarin_maps[token].add(i)\n",
    "print(f\"Abandon tokens:\", mandarin_abandon_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset got 1301 tokens.\n",
      "mandarin got 780 tokens.\n"
     ]
    }
   ],
   "source": [
    "dataset_has_phome: typing.Set[str] = set()\n",
    "for token, line_nums in agresive_maps.items():\n",
    "\tif len(line_nums) > 0:\n",
    "\t\tdataset_has_phome.add(token)\n",
    "print(f\"dataset got {len(dataset_has_phome)} tokens.\")\n",
    "mandarin_dataset_has_phome: typing.Set[str] = set()\n",
    "for token, line_nums in mandarin_maps.items():\n",
    "\tif len(line_nums) > 0:\n",
    "\t\tmandarin_dataset_has_phome.add(token)\n",
    "print(f\"mandarin got {len(mandarin_dataset_has_phome)} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man_has_but_baker_not_has: 11\n"
     ]
    }
   ],
   "source": [
    "man_has_but_baker_not_has = mandarin_dataset_has_phome - dataset_has_phome\n",
    "print(f\"man_has_but_baker_not_has: {len(man_has_but_baker_not_has)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['huo', 'ma', 'er', 'zi', 'men']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_list = list(man_has_but_baker_not_has)\n",
    "diff_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('天气逐渐变冷，要注意穿暖和的衣服。',\n",
       " 'tian1 qi4 zhu2 jian4 bian4 leng3 yao4 zhu4 yi4 chuan1 nuan3 huo de yi1 fu2')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mandarin_dataset[list(mandarin_maps[\"huo\"])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lens: 1414.0\n"
     ]
    }
   ],
   "source": [
    "final_output: typing.List[str] = []\n",
    "for baker_text, baker_phomes in outputs:\n",
    "\tfinal_output.append(baker_text)\n",
    "\tfinal_output.append(baker_phomes)\n",
    "for mandarin_token in diff_list:\n",
    "\t(text, phome) = mandarin_dataset[list(mandarin_maps[mandarin_token])[0]]\n",
    "\tfinal_output.append(text)\n",
    "\tfinal_output.append(phome)\n",
    "print(f\"lens: {len(final_output) / 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "with open(\"assets/text/full_mandarin.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tf.write(\"\\n\".join(final_output))\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 1414\n"
     ]
    }
   ],
   "source": [
    "dataset: typing.List[typing.Tuple[str, str]] = []\n",
    "with open(\"assets/text/full_mandarin.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "\tlines = f.readlines()\n",
    "for i in range(0, len(lines), 2):\n",
    "\ttext = lines[i].strip().strip('\\n')\n",
    "\tphome = lines[i + 1].strip().strip('\\n')\n",
    "\tdataset.append((text, phome))\n",
    "print(f\"len: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('躺在急救担架上的男子双目紧闭，头发散发出一股烧焦的味道。',\n",
       "  'tang3 zai4 ji2 jiu4 dan1 jia4 shang4 de5 nan2 zi3 shuang1 mu4 jin3 bi4 tou2 fa4 san4 fa1 chu1 yi4 gu3 shao1 jiao1 de5 wei4 dao4'),\n",
       " ('工业园区是承接产业转移、加速产业集聚、培育产业集群的主要载体。',\n",
       "  'gong1 ye4 yuan2 qu1 shi4 cheng2 jie1 chan3 ye4 zhuan3 yi2 jia1 su4 chan3 ye4 ji2 ju4 pei2 yu4 chan3 ye4 ji2 qun2 de5 zhu3 yao4 zai4 ti3'),\n",
       " ('那一刻，我才真正的懂你，就像懂我现在的自己。',\n",
       "  'na4 yi2 ke4 wo3 cai2 zhen1 zheng4 de5 dong2 ni3 jiu4 xiang4 dong2 wo3 xian4 zai4 de5 zi4 ji3'),\n",
       " ('由于列车长时间停靠，车厢内的空气越来越“闷”。',\n",
       "  'you2 yu2 lie4 che1 zhang3 shi2 jian1 ting2 kao4 che1 xiang1 nei4 de5 kong1 qi4 yue4 lai2 yue4 men1'),\n",
       " ('但如果按车队规模，一嗨数千辆车的量级绝对算不上最大。',\n",
       "  'dan4 ru2 guo3 an4 che1 dui4 gui1 mo2 yi4 hai1 shu4 qian1 liang4 che1 de5 liang4 ji2 jue2 dui4 suan4 bu2 shang4 zui4 da4')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got token set 2069\n"
     ]
    }
   ],
   "source": [
    "token_filename = \"assets/dataset/tokens.txt\"\n",
    "token_set: typing.Set[str] = set()\n",
    "with open(token_filename, 'r', encoding='utf-8') as f:\n",
    "\tfor line in f.readlines():\n",
    "\t\ttoken = line.strip('\\n').strip().split(' ')[0]\n",
    "\t\ttoken_set.add(token)\n",
    "print(f\"Got token set {len(token_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abandon tokens: {'kan5', 'jun5', 'jinr4', 'wu5', 'teng5', 'na5', 'suo5', 'niang5', 'lai5', 'ti5', 'la5', 'jia5', 'tao5', 'fen5', 'bo5', 'jing5', 'hair2', 'qi5', 'cha5', 'mei5', 'nai5', 'hua5', 'ju5', 'dianr3', 'die5', 'ba5', 'ji5', 'mi5', 'ger4', 'liang5', 'fur4', 'sha5', 'huir4', 'cao5', 'bang5', 'qu5', 'ma5', 'shao5', 'kuai5', 'shei2', 'gua5', 'ng1', 'bao5', 'ter4', 'xing5', 'bai5', 'shi5', 'fang5', 'cir2', 'zhe5', 'long5', 'zhi5', 'you5', 'pi5', 'heng5', 'dou5', 'tan5', 'ye5', 'zha5', 'hao5', 'menr5', 'qing5', 'da5', 'zu5', 'luo5', 'tu5', 'xi5', 'le5', 'du5', 'zhei4', 'tang5', 'zhou5', 'su5', 'shuo5', 'qin5', 'pan5', 'tuo5', 'yuan5', 'pai5', 'mao5', 'sao5', 'peng5', 'xie5', 'hu5', 'guo5', 'fu5', 'er5', 'lu5', 'kou5', 'niu5', 'gu5', 'jiu5', 'yang5', 'sou5', 'tou5', 'ya5', 'duo5', 'niur1', 'se5', 'qiu5', 'chan5', 'kuair4', 'sheng5', 'dao5', 'ha5', 'o5', 'yi5', 'rang5', 'tongr4', 'shuan5', 'a5', 'qie5', 'ne5', 'sa5', 'si5', 'li5', 'bu5', 'zi5', 'nan5', 'huan5', 'hou5', 'huo5', 'yir4', 'shen5', 'ling5', 'cai5', 'fa5', 'ge5', 'yong5', 'tanr1', 'he5', 'yanr2', 'me5', 'ren5', 'di5', 'shang5', 'bian5', 'wan5', 'po5', 'tai5', 'jie5', 'wa5', 'men5', 'gong5', 'nang5', 'pa5', 'cong5', 'de5', 'lan5', 'ying5', 'tun5', 'nar4', 'shir4'}\n"
     ]
    }
   ],
   "source": [
    "maps: typing.Dict[str, typing.Set[int]] = { x: set() for x in token_set } # { token: [line number] }\n",
    "abandon_token: typing.Set[str] = set()\n",
    "for i, (text, phome) in enumerate(dataset):\n",
    "\tfor token in phome.split(' '):\n",
    "\t\tif token not in token_set:\n",
    "\t\t\tabandon_token.add(token)\n",
    "\t\telse:\n",
    "\t\t\tmaps[token].add(i)\n",
    "print(f\"Abandon tokens:\", abandon_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 100, 100, 100]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [ 0 for _ in range(len(dataset))]\n",
    "for i, (text, phome) in enumerate(dataset):\n",
    "\tfor token in phome.split(' '):\n",
    "\t\tif token in maps:\n",
    "\t\t\tif len(maps[token]) == 1:\n",
    "\t\t\t\tscores[i] += 100\n",
    "\t\t\telif len(maps[token]) == 2:\n",
    "\t\t\t\tscores[i] += 1\n",
    "scores[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agresive_maps: typing.Dict[str, typing.List[int]] = {} # token: [line number]\n",
    "for (key, value) in maps.items():\n",
    "\tline_list = list(value)\n",
    "\tline_list.sort(key=lambda x: scores[x], reverse=True)\n",
    "\tagresive_maps[key] = line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voicelab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
