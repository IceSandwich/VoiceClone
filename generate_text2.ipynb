{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, typing, jieba, pypinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 1414\n"
     ]
    }
   ],
   "source": [
    "dataset: typing.List[typing.Tuple[str, str]] = []\n",
    "with open(\"assets/text/full_mandarin.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "\tlines = f.readlines()\n",
    "for i in range(0, len(lines), 2):\n",
    "\ttext = lines[i].strip().strip('\\n')\n",
    "\tphome = lines[i + 1].strip().strip('\\n')\n",
    "\tdataset.append((text, phome))\n",
    "print(f\"len: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('躺在急救担架上的男子双目紧闭，头发散发出一股烧焦的味道。',\n",
       "  'tang3 zai4 ji2 jiu4 dan1 jia4 shang4 de5 nan2 zi3 shuang1 mu4 jin3 bi4 tou2 fa4 san4 fa1 chu1 yi4 gu3 shao1 jiao1 de5 wei4 dao4'),\n",
       " ('工业园区是承接产业转移、加速产业集聚、培育产业集群的主要载体。',\n",
       "  'gong1 ye4 yuan2 qu1 shi4 cheng2 jie1 chan3 ye4 zhuan3 yi2 jia1 su4 chan3 ye4 ji2 ju4 pei2 yu4 chan3 ye4 ji2 qun2 de5 zhu3 yao4 zai4 ti3'),\n",
       " ('那一刻，我才真正的懂你，就像懂我现在的自己。',\n",
       "  'na4 yi2 ke4 wo3 cai2 zhen1 zheng4 de5 dong2 ni3 jiu4 xiang4 dong2 wo3 xian4 zai4 de5 zi4 ji3'),\n",
       " ('由于列车长时间停靠，车厢内的空气越来越“闷”。',\n",
       "  'you2 yu2 lie4 che1 zhang3 shi2 jian1 ting2 kao4 che1 xiang1 nei4 de5 kong1 qi4 yue4 lai2 yue4 men1'),\n",
       " ('但如果按车队规模，一嗨数千辆车的量级绝对算不上最大。',\n",
       "  'dan4 ru2 guo3 an4 che1 dui4 gui1 mo2 yi4 hai1 shu4 qian1 liang4 che1 de5 liang4 ji2 jue2 dui4 suan4 bu2 shang4 zui4 da4')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got token set 2069\n"
     ]
    }
   ],
   "source": [
    "token_filename = \"assets/dataset/tokens.txt\"\n",
    "token_set: typing.Set[str] = set()\n",
    "with open(token_filename, 'r', encoding='utf-8') as f:\n",
    "\tfor line in f.readlines():\n",
    "\t\ttoken = line.strip('\\n').strip().split(' ')[0]\n",
    "\t\ttoken_set.add(token)\n",
    "print(f\"Got token set {len(token_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abandon tokens: {'cha5', 'wan5', 'ger4', 'yanr2', 'da5', 'ren5', 'ye5', 'xi5', 'nang5', 'huan5', 'teng5', 'duo5', 'de5', 'yir4', 'ling5', 'shuan5', 'lai5', 'wa5', 'hou5', 'qin5', 'xie5', 'yang5', 'sha5', 'hu5', 'pi5', 'zi5', 'tou5', 'ji5', 'bian5', 'sheng5', 'ju5', 'die5', 'ya5', 'nai5', 'hair2', 'dou5', 'ter4', 'jiu5', 'qing5', 'di5', 'he5', 'guo5', 'tang5', 'ng1', 'wu5', 'shuo5', 'mei5', 'kan5', 'jinr4', 'bu5', 'heng5', 'kou5', 'you5', 'gu5', 'cao5', 'pa5', 'dianr3', 'peng5', 'qu5', 'qiu5', 'ne5', 'chan5', 'kuair4', 'mao5', 'ba5', 'fa5', 'ying5', 'tun5', 'er5', 'yong5', 'pai5', 'na5', 'fang5', 'lan5', 'luo5', 'hao5', 'tuo5', 'sou5', 'niang5', 'jie5', 'liang5', 'cai5', 'la5', 'sa5', 'xing5', 'zhi5', 'suo5', 'menr5', 'se5', 'shang5', 'tao5', 'long5', 'rang5', 'zhou5', 'gua5', 'nan5', 'bao5', 'bai5', 'sao5', 'ge5', 'jia5', 'le5', 'po5', 'kuai5', 'ha5', 'lu5', 'yi5', 'hua5', 'qie5', 'ti5', 'tan5', 'jun5', 'zha5', 'huir4', 'mi5', 'pan5', 'dao5', 'nar4', 'ma5', 'zu5', 'su5', 'tanr1', 'zhe5', 'cong5', 'shei2', 'fen5', 'fur4', 'gong5', 'tu5', 'jing5', 'niu5', 'du5', 'bo5', 'shir4', 'shao5', 'shi5', 'men5', 'tai5', 'cir2', 'zhei4', 'qi5', 'a5', 'bang5', 'si5', 'huo5', 'yuan5', 'tongr4', 'o5', 'fu5', 'niur1', 'li5', 'shen5', 'me5'}\n"
     ]
    }
   ],
   "source": [
    "maps: typing.Dict[str, typing.Set[int]] = { x: set() for x in token_set } # { token: [line number] }\n",
    "abandon_token: typing.Set[str] = set()\n",
    "for i, (text, phome) in enumerate(dataset):\n",
    "\tfor token in phome.split(' '):\n",
    "\t\tif token not in token_set:\n",
    "\t\t\tabandon_token.add(token)\n",
    "\t\telse:\n",
    "\t\t\tmaps[token].add(i)\n",
    "print(f\"Abandon tokens:\", abandon_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 100, 100, 100]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [ 0 for _ in range(len(dataset))]\n",
    "for i, (text, phome) in enumerate(dataset):\n",
    "\tfor token in phome.split(' '):\n",
    "\t\tif token in maps:\n",
    "\t\t\tif len(maps[token]) == 1:\n",
    "\t\t\t\tscores[i] += 100\n",
    "\t\t\telif len(maps[token]) == 2:\n",
    "\t\t\t\tscores[i] += 1\n",
    "scores[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agresive_maps: typing.Dict[str, typing.List[int]] = {} # token: [line number]\n",
    "for (key, value) in maps.items():\n",
    "\tline_list = list(value)\n",
    "\tline_list.sort(key=lambda x: scores[x], reverse=True)\n",
    "\tagresive_maps[key] = line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got outputs: 1205\n"
     ]
    }
   ],
   "source": [
    "n_select = 3\n",
    "select_set: typing.Set[int] = set()\n",
    "for key, line_numbers in agresive_maps.items():\n",
    "\tfor i in line_numbers[:n_select]:\n",
    "\t\tselect_set.add(i)\n",
    "outputs = [ dataset[i] for i in select_set]\n",
    "print(f\"Got outputs: {len(outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('躺在急救担架上的男子双目紧闭，头发散发出一股烧焦的味道。',\n",
       "  'tang3 zai4 ji2 jiu4 dan1 jia4 shang4 de5 nan2 zi3 shuang1 mu4 jin3 bi4 tou2 fa4 san4 fa1 chu1 yi4 gu3 shao1 jiao1 de5 wei4 dao4'),\n",
       " ('工业园区是承接产业转移、加速产业集聚、培育产业集群的主要载体。',\n",
       "  'gong1 ye4 yuan2 qu1 shi4 cheng2 jie1 chan3 ye4 zhuan3 yi2 jia1 su4 chan3 ye4 ji2 ju4 pei2 yu4 chan3 ye4 ji2 qun2 de5 zhu3 yao4 zai4 ti3'),\n",
       " ('那一刻，我才真正的懂你，就像懂我现在的自己。',\n",
       "  'na4 yi2 ke4 wo3 cai2 zhen1 zheng4 de5 dong2 ni3 jiu4 xiang4 dong2 wo3 xian4 zai4 de5 zi4 ji3'),\n",
       " ('由于列车长时间停靠，车厢内的空气越来越“闷”。',\n",
       "  'you2 yu2 lie4 che1 zhang3 shi2 jian1 ting2 kao4 che1 xiang1 nei4 de5 kong1 qi4 yue4 lai2 yue4 men1'),\n",
       " ('但如果按车队规模，一嗨数千辆车的量级绝对算不上最大。',\n",
       "  'dan4 ru2 guo3 an4 che1 dui4 gui1 mo2 yi4 hai1 shu4 qian1 liang4 che1 de5 liang4 ji2 jue2 dui4 suan4 bu2 shang4 zui4 da4')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converage: 1312 / 2069\n",
      "dataset: 1205\n"
     ]
    }
   ],
   "source": [
    "output_set: typing.Set[str] = set()\n",
    "for text, phomes in outputs:\n",
    "\tfor phome in phomes.split(' '):\n",
    "\t\toutput_set.add(phome)\n",
    "print(f\"converage: {len(token_set) - len(token_set - output_set)} / {len(token_set)}\")\n",
    "print(f\"dataset: {len(outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converage: 1312 / 2069\n",
      "dataset: 1414\n"
     ]
    }
   ],
   "source": [
    "dataset_set: typing.Set[str] = set()\n",
    "for text, phomes in outputs:\n",
    "\tfor phome in phomes.split(' '):\n",
    "\t\tdataset_set.add(phome)\n",
    "print(f\"converage: {len(token_set) - len(token_set - dataset_set)} / {len(token_set)}\")\n",
    "print(f\"dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "with open(\"assets/text/mandarin_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tfor text, phome in outputs:\n",
    "\t\tf.write(f\"{text}\\n{phome}\\n\")\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unselected_set = set(range(len(dataset))) - select_set\n",
    "unselected_dataset = [ dataset[i] for i in unselected_set ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_range = len(unselected_set) // 4\n",
    "select_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('他决定利用业余时间宣传脆骨病，让大家关注脆弱的“瓷娃娃”。',\n",
       "  'ta1 jue2 ding4 li4 yong4 ye4 yu2 shi2 jian1 xuan1 chuan2 cui4 gu3 bing4 rang4 da4 jia1 guan1 zhu4 cui4 ruo4 de5 ci2 wa2 wa5'),\n",
       " ('此间，内陆各省为了争上内陆第一核电站而拼得头破血流。',\n",
       "  'ci3 jian1 nei4 lu4 ge4 sheng3 wei4 le5 zheng1 shang4 nei4 lu4 di4 yi1 he2 dian4 zhan4 er2 pin1 de5 tou2 po4 xue4 liu2'),\n",
       " ('最后，他的目光停留在那几个大字上：南无阿弥陀佛。',\n",
       "  'zui4 hou4 ta1 de5 mu4 guang1 ting2 liu2 zai4 na4 ji3 ge5 da4 zi4 shang4 nan2 wu2 e1 mi5 tuo2 fo2'),\n",
       " ('早上在楼下汤粉店吃早餐，碰见我们领导，并热情的打招呼。',\n",
       "  'zao3 shang4 zai4 lou2 xia4 tang1 fen3 dian4 chi1 zao3 can1 peng4 jian4 wo3 men5 ling2 dao3 bing4 re4 qing2 de5 da3 zhao1 hu5'),\n",
       " ('只要不引起笑场，快男、超女式的造型唱京剧也无妨，观众才是上帝。',\n",
       "  'zhi3 yao4 bu4 yin2 qi3 xiao4 chang3 kuai4 nan2 chao1 nv3 shi4 de5 zao4 xing2 chang4 jing1 ju4 ye3 wu2 fang2 guan1 zhong4 cai2 shi4 shang4 di4')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset = random.sample(unselected_dataset, k=select_range)\n",
    "valid_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "with open(\"assets/text/mandarin_valid.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tfor text, phome in valid_dataset:\n",
    "\t\tf.write(f\"{text}\\n{phome}\\n\")\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voicelab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
