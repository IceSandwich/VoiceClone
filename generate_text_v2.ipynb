{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, typing, jieba, pypinyin, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset(dataset: typing.List[typing.Tuple[str, str]], n: int = 5):\n",
    "    print(f\"Data samples (length = {len(dataset)}): [\")\n",
    "    for i in range(n):\n",
    "        print(f'\\t{dataset[i]},')\n",
    "    print(f', ...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 压缩数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BZNSYP has 10000 datas.\n",
      "Data samples: [\n",
      "\t('卡尔普陪外孙玩滑梯。', 'ka2 er2 pu3 pei2 wai4 sun1 wan2 hua2 ti1'),\n",
      "\t('假语村言别再拥抱我。', 'jia2 yu3 cun1 yan2 bie2 zai4 yong1 bao4 wo3'),\n",
      "\t('宝马配挂跛骡鞍，貂蝉怨枕董翁榻。', 'bao2 ma3 pei4 gua4 bo3 luo2 an1 diao1 chan2 yuan4 zhen3 dong3 weng1 ta4'),\n",
      "\t('邓小平与撒切尔会晤。', 'deng4 xiao3 ping2 yu3 sa4 qie4 er3 hui4 wu4'),\n",
      "\t('老虎幼崽与宠物犬玩耍。', 'lao2 hu3 you4 zai3 yu2 chong3 wu4 quan3 wan2 shua3'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "def parse_bznsyp_dataset(filename: str):\n",
    "    dataset: typing.List[typing.Tuple[str, str]]  = []\n",
    "    regex = re.compile('#\\d+')\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(0, len(lines), 2):\n",
    "            splitpos = lines[i].index('\\t')\n",
    "            text = lines[i][splitpos+1:].strip('\\n')\n",
    "            text = regex.sub(\"\", text)\n",
    "\n",
    "            phome = lines[i + 1].strip('\\n').strip()\n",
    "            dataset.append((text, phome))\n",
    "    return dataset\n",
    "\n",
    "bznsyp_ds = parse_bznsyp_dataset(\"assets/text/BZNSYP/000001-010000.txt\")\n",
    "print(f\"BZNSYP has {len(bznsyp_ds)} datas.\")\n",
    "print_dataset(bznsyp_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BZNSYP has 1607 tokens.\n",
      "Most active tokens: [('zai4', 1428), ('le5', 1530), ('yi4', 1653), ('shi4', 2512), ('de5', 4219)]\n",
      "Deactive tokens: [('wanr1', 1), ('jir1', 1), ('jiang5', 1), ('tie4', 1), ('zuo5', 1)]\n"
     ]
    }
   ],
   "source": [
    "def analysis_token_frequency(dataset: typing.List[typing.Tuple[str, str]]):\n",
    "    maps: typing.Dict[str, typing.Set[int]] = {} # { token: [line numbers] }\n",
    "    for i, (text, phome) in enumerate(dataset):\n",
    "        for token in phome.split(' '):\n",
    "            if token in maps:\n",
    "                maps[token].add(i)\n",
    "            else:\n",
    "                maps[token] = set([i])\n",
    "    return maps\n",
    "\n",
    "bznsyp_analysis = analysis_token_frequency(bznsyp_ds)\n",
    "bznsyp_tokens_counts = sorted([ (key, len(value)) for key, value in bznsyp_analysis.items() ], key=lambda x: x[1])\n",
    "print(f\"BZNSYP has {len(bznsyp_analysis)} tokens.\")\n",
    "print(f\"Most active tokens: {bznsyp_tokens_counts[-5:]}\")\n",
    "print(f\"Deactive tokens: {bznsyp_tokens_counts[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got pretrained tokens: 2069\n"
     ]
    }
   ],
   "source": [
    "def parse_token_file(filename: str):\n",
    "    token_list: typing.List[str] = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            token = line.strip().strip('\\n').split(' ')[0]\n",
    "            token_list.append(token)\n",
    "    return token_list\n",
    "\n",
    "tokens = parse_token_file(\"assets/dataset/tokens.txt\")\n",
    "print(f\"Got pretrained tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens that BZNSYP has but `tokens` hasn't: 306\n",
      "Tokens that `tokens` has but BZNSYP hasn't: 768\n",
      "Tokens that both have: 1301\n"
     ]
    }
   ],
   "source": [
    "bznsyp_set = set(list(bznsyp_analysis.keys()))\n",
    "tokens_set = set(tokens)\n",
    "\n",
    "print(f\"Tokens that BZNSYP has but `tokens` hasn't: {len(bznsyp_set - tokens_set)}\")\n",
    "print(f\"Tokens that `tokens` has but BZNSYP hasn't: {len(tokens_set - bznsyp_set)}\")\n",
    "print(f\"Tokens that both have: {len(bznsyp_set & tokens_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_bznsyp_tokens(tokens: typing.List[str]):\n",
    "    ret: typing.List[str] = []\n",
    "    for token in tokens:\n",
    "        if token.endswith('5'): # 5音是轻声\n",
    "            ret.append(token[:-1])\n",
    "        elif len(token) >= 3 and token[-2] == 'r' and token[:2] != \"er\" and not token.startswith('letter'): # 儿音， fur4 => fu4 er2\n",
    "            ret.append(token[:-2])\n",
    "            ret.append('er2')\n",
    "        elif token in [\"。\", \"，\", \"—\", \"“\", \"”\", \"？\", \"！\", \"：\", \"、\", \"；\", \"…\"]:\n",
    "            continue # ignore\n",
    "        else:\n",
    "            ret.append(token)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abandon tokens: {'shei2', 'P', 'menr', 'zhei4', 'ng1', 'tei1', 'yir'}\n",
      "len: 2069\n"
     ]
    }
   ],
   "source": [
    "def fit_dataset_to_token_set(dataset: typing.List[typing.Tuple[str, str]], tokens_set: typing.Set[str]):\n",
    "    maps: typing.Dict[str, typing.Set[int]] = { x: set() for x in tokens_set } # { token: [line number] }\n",
    "    abandon_token: typing.Set[str] = set()\n",
    "    for i, (text, phome) in enumerate(dataset):\n",
    "        prepare_to_add: typing.List[str] = []\n",
    "        skip_this_row = False\n",
    "        for token in fix_bznsyp_tokens(phome.split(' ')):\n",
    "            if token not in tokens_set: \n",
    "                abandon_token.add(token)\n",
    "                skip_this_row = True\n",
    "                break\n",
    "            else:\n",
    "                prepare_to_add.append(token)\n",
    "        if not skip_this_row:\n",
    "            for token in prepare_to_add:\n",
    "                maps[token].add(i)\n",
    "    print(f\"Abandon tokens:\", abandon_token)\n",
    "    return maps\n",
    "\n",
    "token_to_bznsyp_line = fit_dataset_to_token_set(bznsyp_ds, tokens_set)\n",
    "print(f\"len: {len(token_to_bznsyp_line)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_character(text: str):\n",
    "    return text.replace('，','').replace('。','').replace('—','').replace(\"“\",'').replace(\"”\",'').replace('？','').replace('！','').replace('：','').replace('！','').replace('、','').replace('；','').replace('…','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_length(dataset: typing.List[typing.Tuple[str, str]], fit_table: typing.Dict[str, typing.List[int]]):\n",
    "    ret: typing.Dict[str, typing.List[int]] = {}\n",
    "    for token, line_numbers in fit_table.items():\n",
    "        scores = [ 0 for _ in range(len(line_numbers)) ]\n",
    "        for i, line in enumerate(line_numbers):\n",
    "            text = dataset[line][0]\n",
    "            length = len(remove_non_character(text))\n",
    "            scores[i] = length\n",
    "        ret[token] = [ x[1] for x in sorted(zip(range(len(line_numbers)), line_numbers), key=lambda i: scores[i[0]], reverse=True) ]\n",
    "    return ret\n",
    "\n",
    "def sort_by_freq(dataset: typing.List[typing.Tuple[str, str]], fit_table: typing.Dict[str, typing.List[int]]):\n",
    "    ret: typing.Dict[str, typing.List[int]] = {}\n",
    "    for token, line_numbers in fit_table.items():\n",
    "        scores = [ 0 for _ in range(len(line_numbers)) ]\n",
    "        for i, line in enumerate(line_numbers):\n",
    "            for token in fix_bznsyp_tokens(dataset[line][1].split(' ')):\n",
    "                if token in fit_table:\n",
    "                    if len(fit_table[token]) == 1: # 稀有的token需要更容易选中\n",
    "                        scores[i] += 100\n",
    "                    elif len(fit_table[token]) == 2:\n",
    "                        scores[i] += 1\n",
    "        ret[token] = [ x[1] for x in sorted(zip(range(len(line_numbers)), line_numbers), key=lambda i: scores[i[0]], reverse=True) ]\n",
    "    return ret\n",
    "\n",
    "def select_first_n(fit_table: typing.Dict[str, typing.List[int]], n: int):\n",
    "    ret: typing.Dict[str, typing.List[int]] = {}\n",
    "    for token, line_numbers in fit_table.items():\n",
    "        for line in line_numbers[:n]:\n",
    "            if token not in ret:\n",
    "                ret[token] = [line]\n",
    "            else:\n",
    "                ret[token].append(line)\n",
    "    return ret\n",
    "\n",
    "def flatten_select_result(select_result: typing.Dict[str, typing.List[int]]):\n",
    "    select_sets: typing.Set[int] = set()\n",
    "    for key, value in select_result.items():\n",
    "        for v in value:\n",
    "            select_sets.add(v)\n",
    "    return select_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset token converage: 1215 compare to original 1301\n",
      "New dataset counts: 950 compare to original 10000\n",
      "Data samples (length = 950): [\n",
      "\t('卡尔普陪外孙玩滑梯。', 'ka2 er2 pu3 pei2 wai4 sun1 wan2 hua2 ti1'),\n",
      "\t('苦涩的沙吹痛脸庞的感觉。', 'ku3 se4 de5 sha1 chui1 tong4 lian3 pang2 de5 gan3 jue2'),\n",
      "\t('宝马配挂跛骡鞍，貂蝉怨枕董翁榻。', 'bao2 ma3 pei4 gua4 bo3 luo2 an1 diao1 chan2 yuan4 zhen3 dong3 weng1 ta4'),\n",
      "\t('在“三嬢”陈章淑眼里，媛媛懂事、嘴甜、聪明，见人就会喊。', 'zai4 san1 niang2 chen2 shu1 zhang1 yan2 li3 yuan2 yuan5 dong3 shi4 zui3 tian2 cong1 ming5 jian4 ren2 jiu4 hui4 han3'),\n",
      "\t('老虎幼崽与宠物犬玩耍。', 'lao2 hu3 you4 zai3 yu2 chong3 wu4 quan3 wan2 shua3'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "sorted_bznsyp_ds = { key: list(value) for key, value in token_to_bznsyp_line.items() }\n",
    "# sorted_bznsyp_ds = sort_by_length(bznsyp_ds, sorted_bznsyp_ds)\n",
    "# sorted_bznsyp_ds = select_first_n(sorted_bznsyp_ds, 4)\n",
    "sorted_bznsyp_ds = sort_by_freq(bznsyp_ds, sorted_bznsyp_ds)\n",
    "sorted_bznsyp_ds = select_first_n(sorted_bznsyp_ds, 3)\n",
    "\n",
    "selected_dataset = [ bznsyp_ds[i] for i in flatten_select_result(sorted_bznsyp_ds) ]\n",
    "subset_set = set(list(analysis_token_frequency(selected_dataset).keys()))\n",
    "print(f\"New dataset token converage: {len(tokens_set & subset_set)} compare to original {len(tokens_set & bznsyp_set)}\")\n",
    "print(f\"New dataset counts: {len(selected_dataset)} compare to original {len(bznsyp_ds)}\")\n",
    "print_dataset(selected_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset: [\n",
      "\t('卡尔普陪外孙玩滑梯。', 'ka2 er2 pu3 pei2 wai4 sun1 wan2 hua2 ti1 。'),\n",
      "\t('苦涩的沙吹痛脸庞的感觉。', 'ku3 se4 de sha1 chui1 tong4 lian3 pang2 de gan3 jue2 。'),\n",
      "\t('宝马配挂跛骡鞍，貂蝉怨枕董翁榻。', 'bao2 ma3 pei4 gua4 bo3 luo2 an1 ， diao1 chan2 yuan4 zhen3 dong3 weng1 ta4 。'),\n",
      "\t('在“三嬢”陈章淑眼里，媛媛懂事、嘴甜、聪明，见人就会喊。', 'zai4 “ san1 niang2 ” chen2 shu1 zhang1 yan2 li3 ， yuan2 yuan dong3 shi4 、 zui3 tian2 、 cong1 ming ， jian4 ren2 jiu4 hui4 han3 。'),\n",
      "\t('老虎幼崽与宠物犬玩耍。', 'lao2 hu3 you4 zai3 yu2 chong3 wu4 quan3 wan2 shua3 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "def add_extra_character_to_phomes(dataset: typing.List[typing.Tuple[str, str]]):\n",
    "    modified_ds = dataset.copy()\n",
    "    for k, (text, phome) in enumerate(dataset):\n",
    "        newphome = []\n",
    "        phomes = fix_bznsyp_tokens(phome.split(' '))\n",
    "        debug_phomes = phomes.copy()\n",
    "        skip_one = False\n",
    "        unhandle = False\n",
    "        for i, ch in enumerate(text):\n",
    "            if skip_one:\n",
    "                skip_one = False\n",
    "                continue\n",
    "            if ch in [\"。\", \"，\", \"—\", \"“\", \"”\", \"？\", \"！\", \"：\", \"、\", \"；\", \"…\"]:\n",
    "                newphome.append(ch)\n",
    "            else:\n",
    "                if len(phomes) == 0:\n",
    "                    unhandle = True\n",
    "                    break\n",
    "                p = phomes.pop(0)\n",
    "                newphome.append(p)\n",
    "        if len(phomes) == 0 and not unhandle:\n",
    "            modified_ds[k] = (text, ' '.join(newphome))\n",
    "        else:\n",
    "            print(f\"Unhandle: {k}\\n== {text}\\n== {debug_phomes}\\nremain: {phomes}\\n\")\n",
    "    return modified_ds\n",
    "\n",
    "cleaned_ds = add_extra_character_to_phomes(selected_dataset)\n",
    "print(f\"Got final dataset.\")\n",
    "print_dataset(cleaned_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def save_v2_dataset(filename: str, dataset: typing.List[typing.Tuple[str, str]]):\n",
    "\twith open(filename, 'w', encoding='utf-8') as f:\n",
    "\t\tfor (text, phome) in dataset:\n",
    "\t\t\tf.write(f\"{text}\\n{phome}\\n\")\n",
    "\n",
    "save_v2_dataset(\"assets/text/mandarin_v2_train.txt\", cleaned_ds)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合v1数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 950): [\n",
      "\t('卡尔普陪外孙玩滑梯。', 'ka2 er2 pu3 pei2 wai4 sun1 wan2 hua2 ti1 。'),\n",
      "\t('苦涩的沙吹痛脸庞的感觉。', 'ku3 se4 de sha1 chui1 tong4 lian3 pang2 de gan3 jue2 。'),\n",
      "\t('宝马配挂跛骡鞍，貂蝉怨枕董翁榻。', 'bao2 ma3 pei4 gua4 bo3 luo2 an1 ， diao1 chan2 yuan4 zhen3 dong3 weng1 ta4 。'),\n",
      "\t('在“三嬢”陈章淑眼里，媛媛懂事、嘴甜、聪明，见人就会喊。', 'zai4 “ san1 niang2 ” chen2 shu1 zhang1 yan2 li3 ， yuan2 yuan dong3 shi4 、 zui3 tian2 、 cong1 ming ， jian4 ren2 jiu4 hui4 han3 。'),\n",
      "\t('老虎幼崽与宠物犬玩耍。', 'lao2 hu3 you4 zai3 yu2 chong3 wu4 quan3 wan2 shua3 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "def parse_v2_dataset(filename: str):\n",
    "    dataset: typing.List[typing.Tuple[str, str]] = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(0, len(lines), 2):\n",
    "            text = lines[i].strip().strip('\\n')\n",
    "            phome = lines[i+1].strip().strip('\\n')\n",
    "            dataset.append((text, phome))\n",
    "    return dataset\n",
    "\n",
    "cleaned_ds = parse_v2_dataset(\"assets/text/mandarin_v2_train.txt\")\n",
    "print_dataset(cleaned_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 729): [\n",
      "\t('他在家里吃苹果，喝茶，打电话，唱歌。', 'ta1 zai4 jia1 li3 chi1 ping2 guo3 ， he1 cha2 ， da3 dian4 hua4 ， chang4 ge1 。'),\n",
      "\t('我去商店买水果和书，看到小猫在玩。', 'wo3 qu4 shang1 dian4 mai3 shui3 guo3 he2 shu1 ， kan4 dao4 xiao3 mao1 zai4 wan2 。'),\n",
      "\t('小明去北京吃炸酱面，他在公园里看见了许多种类的鸟，甚至还遇到了一只大猩猩。', 'xiao3 ming2 qu4 bei3 jing1 chi1 zha2 jiang4 mian4 ， ta1 zai4 gong1 yuan2 li3 kan4 jian4 le xu3 duo1 zhong3 lei4 de niao3 ， shen4 zhi4 hai2 yu4 dao4 le yi4 zhi1 da4 xing1 xing1 。'),\n",
      "\t('小猫学会了跳舞，爬上了高高的树。', 'xiao3 mao1 xue2 hui4 le tiao4 wu3 ， pa2 shang4 le gao1 gao1 de shu4 。'),\n",
      "\t('风筝在天空中飞得又高又快，真是太漂亮了。', 'feng1 zheng1 zai4 tian1 kong1 zhong1 fei1 de2 you4 gao1 you4 kuai4 ， zhen1 shi4 tai4 piao4 liang4 le 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "def parse_v1_dataset(filename: str):\n",
    "    dataset: typing.List[typing.Tuple[str, str]] = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            text = line.strip().strip('\\n')\n",
    "            if text == \"\": continue\n",
    "            cuts = list(jieba.cut(text))\n",
    "            phome = []\n",
    "            for cut in cuts:\n",
    "                pinyins = pypinyin.lazy_pinyin(cut, style=pypinyin.Style.TONE3, tone_sandhi=True)\n",
    "                for pinyin in pinyins:\n",
    "                    phome.append(pinyin)\n",
    "            phome = ' '.join(phome)\n",
    "            dataset.append((text, phome))\n",
    "    return dataset\n",
    "\n",
    "v1_ds = parse_v1_dataset(\"assets/text/mandarin.txt\")\n",
    "print_dataset(v1_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token converage: 779\n"
     ]
    }
   ],
   "source": [
    "v1_ds_set = set(list(analysis_token_frequency(v1_ds).keys()))\n",
    "print(f\"Token converage: {len(tokens_set & v1_ds_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abandon tokens: {'5G', 'AI', 'GDPR'}\n",
      "Abandon tokens: set()\n"
     ]
    }
   ],
   "source": [
    "fit_table_v1 = fit_dataset_to_token_set(v1_ds, tokens_set)\n",
    "fit_table_cleaned_ds = fit_dataset_to_token_set(cleaned_ds, tokens_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 641 tokens from v1 to v2.\n",
      "Tokens: ['hei3', 'bin', 'hua3', 'ri3', 'nun1', 'dong2', 'wai', 'ha3', 'qiong3', 'zheng', 'ce1', 'kuan', 'reng', 'tu', 'han', 'gen2', 'zan', 'shai1', 'rui4', 'qun4', 'cen1', 'pie4', 'se2', 'zun3', 'm2', 'qiong1', 'rong1', 'za4', 'sen', 'jiong1', 'neng', 'nuo3', 'lve1', 'nian1', 'ga', 'hm2', 'run1', 'ping3', 'cang3', 'keng4', 'wu', 'gao', 'rua4', 'zeng4', 'dia4', 'mou4', 'niao', 'lan1', 'zan1', 'yo3', 'xiang', 'shuang3', 'cong4', 'nou3', 'hm4', 'lu', 'lu1', 'nin', 'men3', 'pang4', 'yue3', 'hong4', 'zei4', 'zuan', 'lai3', 'seng2', 'mie3', 'an', 'ruan1', 'cun', 'nu', 'nv1', 'yao', 'chui3', 'te', 'zen', 'nie2', 'cen2', 'rua2', 'eng1', 'kui', 'wai1', 'zhuo', 'mou1', 'bing', 'ge3', 'rong3', 'cou2', 'den1', 'mang', 'mie2', 'chun4', 'ken1', 'ceng', 'se3', 'n4', 'chuo2', 'nuo1', 'rang3', 'zun2', 'shuo3', 'qia', 'hen', 'duan2', 'cu1', 'teng4', 'hei', 'diu3', 'san', 'ce', 'kang', 'nang3', ':', 'shua4', 'de1', 'nie', 'rao1', 'chua1', 'qiao', 'zhun4', 'ang', 'xin3', 'kuai3', 'chen', 'zang2', 'chui4', 'can2', 'suo4', 'chuo', 'seng', 'fiao3', 'neng4', 'zui1', 'fo3', 'zun', 'kuang1', 'man1', 'fou', 'zha3', 'pa2', 'ze', 'sou3', 'hen4', 'song', 'tui2', 'ru1', 'seng1', 'dun3', 'ken', 'na1', 'dao', 'e3', 'cheng4', 'mang4', 'ne1', 'leng4', 'zhui2', 'bu1', 'a2', 'lei', 'zou', 'chua3', 'lve3', 'rao', 'jue3', 'chai', 'en2', 'leng', 'hei2', 'dang', 'nang2', 'pu', 'rou3', 'hm3', 'pou4', 'fei', 'lve', 'rong', 'kua', 'quan', 'zhuang3', 'gen3', 'weng', 'fiao4', 'diao2', 'pang', 'sai3', 'dia1', 'chao4', 'min', 'ni', 'zhang', 'chuai', 'chuai1', 'niao2', 'beng', 'chuo3', 'juan', 'lai1', 'man', 'deng', 'sen4', 'chai3', 'cou1', 'juan2', 'nou1', 'che', 'ming3', 'beng4', 'ha4', 'nuan1', 'cou3', 'xian', 'shun', '!', 'dei', 'he3', \"'\", 'eng4', 'kuai2', 'me4', 'xiu2', 'ruan', 'zhua', 'rua3', 'ou', 'zhou4', 'hang', 'nen1', 'bin2', 'nuo', 'den4', 'tun1', 'biao', 'gen4', 'cong3', 'gua3', 'zhuai3', 'ca2', 'ca', 'die3', 'nong3', 'le3', 'kao', 'ben1', 'pan', 'pei3', 'tiao', 'chua2', 'tai', 'cou', 'zhou2', 'nai1', 'fo', 'ei1', 'jiong', 'fo4', 'weng4', 'zeng2', 'zhong', 'ding', 'ka', 'zhui', 'can', 'chou4', 'zhua2', 'kun', 'cang4', 'li1', 'me2', 'la2', 'huo1', 'lia4', 'pou2', 'suan2', 'shui', '\"', 'qia3', 'nei2', 'ming1', 'hen1', 'gun1', 'ken4', 'beng3', 'pao', 'o2', 'qiong', 'rou1', 'chuang', 'bi', 'ju', 'hei4', 'miu', 'ku', 'zhai', 'geng', 'bian', 'za3', 'shuan2', 'kai4', 'dia', 'que3', 'den2', 'lou3', 'jin2', 'diu4', 'he', 'reng4', 'zhuang2', 'jin', 'nang', 'nong1', 'hang3', 'eng2', 'nin3', 'ruan4', 'nun', 'cang', 'zei', 'a3', 'tun4', 'sou', 'ri', 'ci1', 'mang1', 'piao', 'shuan3', 'dui2', 'xiong3', '?', 'que', 'zeng', 'leng1', 'meng', 'dei4', 'ang4', 'nou2', 'nou4', 're', 'le2', 'duan', 'zhuai', 'ri1', 'tai3', 'cen4', 'se1', 'nuan', 'miu1', 'bin4', 'chua', 'nv4', 'huang4', 'ruo2', 'xiong4', 'cu', 'nen2', 'xun3', 'mian1', 'ning1', 'ke', 'te1', 'ei2', 'kuo', 'heng3', 'cu3', 'zhua4', 'cen3', 'tian', 'zi2', 'luan', 'chan4', 'nei1', 'diu', 'yo2', 'ka4', 'rong4', 'ei4', 'nun2', 'zeng3', 'fou1', 'she', 'tun3', 'qiong4', 'pou3', 'sun4', 'kuo1', 'cuo3', 'ling1', 'bin3', 'suan3', 'ruo', 'eng3', 'niu4', 'n', 'keng2', 'su3', 'zui', 'rua1', 'ce3', 'qun3', 'kuo2', 'kua4', 'en3', 'wang1', 'kuang3', 'zei1', 'huang3', 'cen', 'hm', 'nou', 'xin2', 'huai3', 'tie', 'ruo3', 'shua', 'qiu4', 'tian4', 'shang3', 'zang', 'tou3', 'rua', 'hou1', '0', 'pen3', 'shan', 'ang3', 'ha', 'mai1', ',', 'shuang', 'long1', 'luan1', 'ran1', 'huai', 'pang3', 'ao', 'o3', 'xin', 'mu1', 'lo1', 'te3', 'niao1', 'pai3', 'shuai2', 'ye1', 'ping4', 'yo4', 'kao2', 'na', 'kuan4', 'ci', 'pou', 'chua4', 'zhao', 'lo4', 'n2', 'keng3', 'dui3', 'jun2', 'yue2', 'min1', 'rui1', 'huai1', 'nin4', 'en', 'ze1', 'eng', 'dai2', 'hm1', 'rang2', 'jun3', 'lian1', 'ru', 'neng3', 'cheng', 'zhen', 'juan3', 'm', 'cuan1', 'dun2', 'chan1', 'ce2', 'qiu', 'sang', 'chong', 'lv1', 'sen2', 'run2', 'tuan3', 'min4', 'bie', 'n1', 'lun', 'cuan3', 'ruo1', 'gei4', 'kuai1', 'pian3', 'zhuai2', 'pen1', 'cuan', 'fou4', 'sou2', 'lia1', 'mie', 'zen1', 'ga4', 'qia2', 'sai2', 'guai', 'cao4', 'miu3', 'ze3', 'jue', 'pa', 'miu2', 'cuo', 'teng1', 'miao1', 'nve3', 'dei1', 'pa3', 'chun', 'ceng3', 'nun3', 'nu1', 'pan3', 'hang4', 'zun1', 'nian', 'xuan', 'zu4', 'za', 'guan', 'zhan', 'den', 'hong', 'wen', 'lo2', 'keng', 'zei3', 'm4', 'sen3', 'pen', 'shai2', 'nen3', 'niang1', 'xue', 'zun4', 'zen4', 'bie3', 'xun', 'ran4', 'chai4', 'zhuo4', 'gei1', 'kun3', 'cun4', 'kang3', 'yin', 'pin', 'yun', 'te2', 'den3', 'm1', 'qiu3', 'shun1', 'fiao2', 'bao', '.', 'm3', 'ca4', 'shai3', 'miao', 'run3', 'ren1', 'zuan3', 'zang3', 'me3', 'lve2', 'hun3', 're1', 'neng1', 'jiong4', 'ning', 'dun1', 'le1', 'zhuan2', 'sui', 'nang4', 'ne3', 'reng3', 'shuo2', 'zhuo3', 'rui', 'shuang4', 'ri2', 'nve2', 'nve1', 'pei', 'nve', 'fiao1', 'pie', 'lo3', 'teng3', 'chang', 'seng3', 'lie3', 'zhuai1', 'de3', 'fo1', 'gei', 'nin1', 'fiao', 'zhui3', 'kuo3', 'xiu', 'chuai2', '_', 'liao1', 'n3', 'nie3', 'qun', 'kong', 'nuan4', 'shui1', 'o4', 'guai2', 'er1', 'lun3', 'ting4', 'xia3', 'nun4', 'seng4', 'kai', 'diu2', 'niang3', 'ei3', 'sun', 'nei', 'liang1']\n"
     ]
    }
   ],
   "source": [
    "append_v1_to_v2_tokens: typing.List[str] = []\n",
    "for token, lines in fit_table_v1.items():\n",
    "    if len(fit_table_cleaned_ds[token]) == 0:\n",
    "        append_v1_to_v2_tokens.append(token)\n",
    "print(f\"Merge {len(append_v1_to_v2_tokens)} tokens from v1 to v2.\")\n",
    "print(f\"Tokens: {append_v1_to_v2_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 24): [\n",
      "\t('明天我们会去爬山，打算带上水和干粮，准备好一整天的活动。', 'ming2 tian1 wo3 men hui4 qu4 pa2 shan1 ， da3 suan4 dai4 shang4 shui3 he2 gan1 liang2 ， zhun3 bei4 hao3 yi1 zheng3 tian1 de huo2 dong4 。'),\n",
      "\t('他们家的花园里种了很多蔬菜和水果，看起来非常绿意盎然。', 'ta1 men jia1 de hua1 yuan2 li3 zhong3 le hen3 duo1 shu1 cai4 he2 shui3 guo3 ， kan4 qi3 lai2 fei1 chang2 lv4 yi4 ang4 ran2 。'),\n",
      "\t('小猫学会了跳舞，爬上了高高的树。', 'xiao3 mao1 xue2 hui4 le tiao4 wu3 ， pa2 shang4 le gao1 gao1 de shu4 。'),\n",
      "\t('我真心希望你能够理解我，尽管我很难表达。', 'wo3 zhen1 xin1 xi1 wang4 ni3 neng2 gou4 li3 jie3 wo3 ， jin2 guan3 wo3 hen3 nan2 biao3 da2 。'),\n",
      "\t('心血管疾病是全球致死率最高的疾病之一，早期筛查至关重要。', 'xin1 xue4 guan3 ji2 bing4 shi4 quan2 qiu2 zhi4 si3 lv4 zui4 gao1 de ji2 bing4 zhi1 yi1 ， zao3 qi1 shai1 cha2 zhi4 guan1 zhong4 yao4 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "sub_fit_table = { key: list(value) for key, value in fit_table_v1.items() if key in append_v1_to_v2_tokens }\n",
    "sub_fit_table = sort_by_freq(v1_ds, sub_fit_table)\n",
    "sub_fit_table = select_first_n(sub_fit_table, 3)\n",
    "sub_v1_ds = [ v1_ds[i] for i in flatten_select_result(sub_fit_table) ]\n",
    "print_dataset(sub_v1_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_v2_dataset(\"assets/text/mandarin_v2_noletters.txt\", sub_v1_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合cross letters数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 21): [\n",
      "\t('从A到B的计划需要C和D的配合，以确保E可以成功执行。', 'cong2 A dao4 B de ji4 hua4 xu1 yao4 C he2 D de pei4 he2 ， yi3 que4 bao3 E ke2 yi3 cheng2 gong1 zhi2 xing2 。'),\n",
      "\t('你从F到G的过程中，H的支持是不可缺少的，尤其是在I阶段。', 'ni3 cong2 F dao4 G de guo4 cheng2 zhong1 ， H de zhi1 chi2 shi4 bu4 ke3 que1 shao3 de ， you2 qi2 shi4 zai4 I jie1 duan4 。'),\n",
      "\t('在项目进行中，J和K的合作将直接影响C的效率。', 'zai4 xiang4 mu4 jin4 xing2 zhong1 ， J he2 K de he2 zuo4 jiang1 zhi2 jie1 ying3 xiang3 C de xiao4 lv4 。'),\n",
      "\t('如果你想提高L到M的速度，N的调整将非常重要。', 'ru2 guo3 ni3 xiang3 ti2 gao1 L dao4 M de su4 du4 ， N de tiao2 zheng3 jiang1 fei1 chang2 zhong4 yao4 。'),\n",
      "\t('在这次活动中，O的出现为P带来了更大的机会。', 'zai4 zhe4 ci4 huo2 dong4 zhong1 ， O de chu1 xian4 wei4 P dai4 lai2 le geng4 da4 de ji1 hui4 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "crossletters_ds = parse_v1_dataset(\"assets/text/crossletters.txt\")\n",
    "print_dataset(crossletters_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 21): [\n",
      "\t('从A到B的计划需要C和D的配合，以确保E可以成功执行。', 'cong2 letterA dao4 letterB de ji4 hua4 xu1 yao4 letterC he2 letterD de pei4 he2 ， yi3 que4 bao3 letterE ke2 yi3 cheng2 gong1 zhi2 xing2 。'),\n",
      "\t('你从F到G的过程中，H的支持是不可缺少的，尤其是在I阶段。', 'ni3 cong2 letterF dao4 letterG de guo4 cheng2 zhong1 ， letterH de zhi1 chi2 shi4 bu4 ke3 que1 shao3 de ， you2 qi2 shi4 zai4 letterI jie1 duan4 。'),\n",
      "\t('在项目进行中，J和K的合作将直接影响C的效率。', 'zai4 xiang4 mu4 jin4 xing2 zhong1 ， letterJ he2 letterK de he2 zuo4 jiang1 zhi2 jie1 ying3 xiang3 letterC de xiao4 lv4 。'),\n",
      "\t('如果你想提高L到M的速度，N的调整将非常重要。', 'ru2 guo3 ni3 xiang3 ti2 gao1 letterL dao4 letterM de su4 du4 ， letterN de tiao2 zheng3 jiang1 fei1 chang2 zhong4 yao4 。'),\n",
      "\t('在这次活动中，O的出现为P带来了更大的机会。', 'zai4 zhe4 ci4 huo2 dong4 zhong1 ， letterO de chu1 xian4 wei4 letterP dai4 lai2 le geng4 da4 de ji1 hui4 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(crossletters_ds)):\n",
    "    text = crossletters_ds[i][0]\n",
    "    phome = crossletters_ds[i][1]\n",
    "    newphome = []\n",
    "    for x in phome.split(' '):\n",
    "        if len(x) == 1 and ord('A') <= ord(x[0]) and ord(x[0]) <= ord('Z'):\n",
    "            newphome.append(f'letter' + x)\n",
    "        else:\n",
    "            newphome.append(x)\n",
    "    crossletters_ds[i] = (text, ' '.join(newphome))\n",
    "print_dataset(crossletters_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合后再压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 995): [\n",
      "\t('卡尔普陪外孙玩滑梯。', 'ka2 er2 pu3 pei2 wai4 sun1 wan2 hua2 ti1 。'),\n",
      "\t('苦涩的沙吹痛脸庞的感觉。', 'ku3 se4 de sha1 chui1 tong4 lian3 pang2 de gan3 jue2 。'),\n",
      "\t('宝马配挂跛骡鞍，貂蝉怨枕董翁榻。', 'bao2 ma3 pei4 gua4 bo3 luo2 an1 ， diao1 chan2 yuan4 zhen3 dong3 weng1 ta4 。'),\n",
      "\t('在“三嬢”陈章淑眼里，媛媛懂事、嘴甜、聪明，见人就会喊。', 'zai4 “ san1 niang2 ” chen2 shu1 zhang1 yan2 li3 ， yuan2 yuan dong3 shi4 、 zui3 tian2 、 cong1 ming ， jian4 ren2 jiu4 hui4 han3 。'),\n",
      "\t('老虎幼崽与宠物犬玩耍。', 'lao2 hu3 you4 zai3 yu2 chong3 wu4 quan3 wan2 shua3 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "total_ds: typing.List[typing.Tuple[str, str]] = []\n",
    "total_ds.extend(cleaned_ds)\n",
    "total_ds.extend(sub_v1_ds)\n",
    "total_ds.extend(crossletters_ds)\n",
    "print_dataset(total_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token counts: 2095\n"
     ]
    }
   ],
   "source": [
    "new_tokens_set = tokens_set.copy()\n",
    "for i in range(ord('A'), ord('Z') + 1):\n",
    "    new_tokens_set.add(f'letter' + chr(i)) # crossletters中新增的tokens\n",
    "print(f\"Token counts: {len(new_tokens_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abandon tokens: set()\n",
      "Data samples (length = 574): [\n",
      "\t('卡尔普陪外孙玩滑梯。', 'ka2 er2 pu3 pei2 wai4 sun1 wan2 hua2 ti1 。'),\n",
      "\t('宝马配挂跛骡鞍，貂蝉怨枕董翁榻。', 'bao2 ma3 pei4 gua4 bo3 luo2 an1 ， diao1 chan2 yuan4 zhen3 dong3 weng1 ta4 。'),\n",
      "\t('在“三嬢”陈章淑眼里，媛媛懂事、嘴甜、聪明，见人就会喊。', 'zai4 “ san1 niang2 ” chen2 shu1 zhang1 yan2 li3 ， yuan2 yuan dong3 shi4 、 zui3 tian2 、 cong1 ming ， jian4 ren2 jiu4 hui4 han3 。'),\n",
      "\t('老虎幼崽与宠物犬玩耍。', 'lao2 hu3 you4 zai3 yu2 chong3 wu4 quan3 wan2 shua3 。'),\n",
      "\t('莫里斯这番赤裸裸的种族主义言论遭到舆论痛批。', 'mo4 li3 si1 zhe4 fan1 chi4 luo2 luo3 de zhong3 zu2 zhu3 yi4 yan2 lun4 zao1 dao4 yu2 lun4 tong4 pi1 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "fit_table_final = fit_dataset_to_token_set(total_ds, new_tokens_set)\n",
    "clean2_ds = { key: list(value) for key, value in fit_table_final.items() }\n",
    "clean2_ds = sort_by_freq(total_ds, clean2_ds)\n",
    "clean2_ds = select_first_n(clean2_ds, 3)\n",
    "final_ds = [ total_ds[i] for i in flatten_select_result(clean2_ds) ]\n",
    "print_dataset(final_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token converage: 1393\n"
     ]
    }
   ],
   "source": [
    "final_ds_set = set(list(analysis_token_frequency(final_ds).keys()))\n",
    "print(f\"Token converage: {len(new_tokens_set & final_ds_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_v2_dataset(\"assets/text/mandarin_v2_train_withletters.txt\", final_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制作验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selective range: 9453\n"
     ]
    }
   ],
   "source": [
    "train_texts: typing.Set[str] = set()\n",
    "for (text, phome) in final_ds:\n",
    "    train_texts.add(text)\n",
    "select_range: typing.List[int] = []\n",
    "for i, (text, phome) in enumerate(bznsyp_ds):\n",
    "    if text not in train_texts:\n",
    "        select_range.append(i)\n",
    "print(f\"Selective range: {len(select_range)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 30): [\n",
      "\t('凶徒们将姜老板和女儿女婿捆了起来，逼他们交钱。', 'xiong1 tu2 men5 jiang1 jiang1 lao2 ban3 he2 nv3 er2 nv3 xu4 kun3 le5 qi3 lai2 bi1 ta1 men5 jiao1 qian2'),\n",
      "\t('警方初步调查，夫妻俩死于一氧化碳中毒。', 'jing3 fang1 chu1 bu4 diao4 cha2 fu1 qi1 lia2 si3 yu2 yi1 yang3 hua4 tan4 zhong4 du2'),\n",
      "\t('首先，记者拨打的是约车电话九六幺零六。', 'shou3 xian1 ji4 zhe3 bo1 da3 de5 shi4 yue1 che1 dian4 hua4 jiu3 liu4 yao1 ling2 liu4'),\n",
      "\t('而“苹果”等纸媒，冗员也不少。', 'er2 ping2 guo3 deng2 zhi3 mei2 rong3 yuan2 ye3 bu4 shao3'),\n",
      "\t('人聪明点，毕竟以后我家的生意，都是要交给女婿管理的。', 'ren2 cong1 ming5 dian3 bi4 jing4 yi3 hou4 wo3 jia1 de5 sheng1 yi4 dou1 shi4 yao4 jiao1 gei2 nv3 xu4 guan2 li3 de5'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "select_idx = random.sample(select_range, 30)\n",
    "valid_ds = [ bznsyp_ds[i] for i in select_idx ]\n",
    "print_dataset(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_v2_dataset(\"assets/text/mandarin_v2_valid.txt\", valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voicelab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
