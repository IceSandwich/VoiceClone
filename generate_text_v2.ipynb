{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, typing, jieba, pypinyin, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset(dataset: typing.List[typing.Tuple[str, str]], n: int = 5):\n",
    "    print(f\"Data samples (length = {len(dataset)}): [\")\n",
    "    for i in range(min(len(dataset), n)):\n",
    "        print(f'\\t{dataset[i]},')\n",
    "    print(f', ...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 压缩数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BZNSYP has 10000 datas.\n",
      "Data samples (length = 10000): [\n",
      "\t('卡尔普陪外孙玩滑梯。', 'ka2 er2 pu3 pei2 wai4 sun1 wan2 hua2 ti1'),\n",
      "\t('假语村言别再拥抱我。', 'jia2 yu3 cun1 yan2 bie2 zai4 yong1 bao4 wo3'),\n",
      "\t('宝马配挂跛骡鞍，貂蝉怨枕董翁榻。', 'bao2 ma3 pei4 gua4 bo3 luo2 an1 diao1 chan2 yuan4 zhen3 dong3 weng1 ta4'),\n",
      "\t('邓小平与撒切尔会晤。', 'deng4 xiao3 ping2 yu3 sa4 qie4 er3 hui4 wu4'),\n",
      "\t('老虎幼崽与宠物犬玩耍。', 'lao2 hu3 you4 zai3 yu2 chong3 wu4 quan3 wan2 shua3'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "def parse_bznsyp_dataset(filename: str):\n",
    "    dataset: typing.List[typing.Tuple[str, str]]  = []\n",
    "    regex = re.compile('#\\d+')\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(0, len(lines), 2):\n",
    "            splitpos = lines[i].index('\\t')\n",
    "            text = lines[i][splitpos+1:].strip('\\n')\n",
    "            text = regex.sub(\"\", text)\n",
    "\n",
    "            phome = lines[i + 1].strip('\\n').strip()\n",
    "            dataset.append((text, phome))\n",
    "    return dataset\n",
    "\n",
    "bznsyp_ds = parse_bznsyp_dataset(\"assets/text/BZNSYP/000001-010000.txt\")\n",
    "print(f\"BZNSYP has {len(bznsyp_ds)} datas.\")\n",
    "print_dataset(bznsyp_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BZNSYP has 1607 tokens.\n",
      "Most active tokens: [('zai4', 1428), ('le5', 1530), ('yi4', 1653), ('shi4', 2512), ('de5', 4219)]\n",
      "Deactive tokens: [('wanr1', 1), ('jir1', 1), ('jiang5', 1), ('tie4', 1), ('zuo5', 1)]\n"
     ]
    }
   ],
   "source": [
    "def analysis_token_frequency(dataset: typing.List[typing.Tuple[str, str]]):\n",
    "    maps: typing.Dict[str, typing.Set[int]] = {} # { token: [line numbers] }\n",
    "    for i, (text, phome) in enumerate(dataset):\n",
    "        for token in phome.split(' '):\n",
    "            if token in maps:\n",
    "                maps[token].add(i)\n",
    "            else:\n",
    "                maps[token] = set([i])\n",
    "    return maps\n",
    "\n",
    "bznsyp_analysis = analysis_token_frequency(bznsyp_ds)\n",
    "bznsyp_tokens_counts = sorted([ (key, len(value)) for key, value in bznsyp_analysis.items() ], key=lambda x: x[1])\n",
    "print(f\"BZNSYP has {len(bznsyp_analysis)} tokens.\")\n",
    "print(f\"Most active tokens: {bznsyp_tokens_counts[-5:]}\")\n",
    "print(f\"Deactive tokens: {bznsyp_tokens_counts[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got pretrained tokens: 2069\n"
     ]
    }
   ],
   "source": [
    "def parse_token_file(filename: str):\n",
    "    token_list: typing.List[str] = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            token = line.strip().strip('\\n').split(' ')[0]\n",
    "            token_list.append(token)\n",
    "    return token_list\n",
    "\n",
    "tokens = parse_token_file(\"assets/dataset/tokens.txt\")\n",
    "print(f\"Got pretrained tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens that BZNSYP has but `tokens` hasn't: 306\n",
      "Tokens that `tokens` has but BZNSYP hasn't: 768\n",
      "Tokens that both have: 1301\n"
     ]
    }
   ],
   "source": [
    "bznsyp_set = set(list(bznsyp_analysis.keys()))\n",
    "tokens_set = set(tokens)\n",
    "\n",
    "print(f\"Tokens that BZNSYP has but `tokens` hasn't: {len(bznsyp_set - tokens_set)}\")\n",
    "print(f\"Tokens that `tokens` has but BZNSYP hasn't: {len(tokens_set - bznsyp_set)}\")\n",
    "print(f\"Tokens that both have: {len(bznsyp_set & tokens_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_bznsyp_tokens(tokens: typing.List[str]):\n",
    "    ret: typing.List[str] = []\n",
    "    for token in tokens:\n",
    "        if token.endswith('5'): # 5音是轻声\n",
    "            ret.append(token[:-1])\n",
    "        elif len(token) >= 3 and token[-2] == 'r' and token[:2] != \"er\" and not token.startswith('letter'): # 儿音， fur4 => fu4 er2\n",
    "            ret.append(token[:-2])\n",
    "            ret.append('er2')\n",
    "        elif token in [\"。\", \"，\", \"—\", \"“\", \"”\", \"？\", \"！\", \"：\", \"、\", \"；\", \"…\"]:\n",
    "            continue # ignore\n",
    "        else:\n",
    "            ret.append(token)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abandon tokens: {'P', 'shei2', 'menr', 'zhei4', 'tei1', 'yir', 'ng1'}\n",
      "len: 2069\n"
     ]
    }
   ],
   "source": [
    "def fit_dataset_to_token_set(dataset: typing.List[typing.Tuple[str, str]], tokens_set: typing.Set[str]):\n",
    "    maps: typing.Dict[str, typing.Set[int]] = { x: set() for x in tokens_set } # { token: [line number] }\n",
    "    abandon_token: typing.Set[str] = set()\n",
    "    for i, (text, phome) in enumerate(dataset):\n",
    "        prepare_to_add: typing.List[str] = []\n",
    "        skip_this_row = False\n",
    "        for token in fix_bznsyp_tokens(phome.split(' ')):\n",
    "            if token not in tokens_set: \n",
    "                abandon_token.add(token)\n",
    "                skip_this_row = True\n",
    "                break\n",
    "            else:\n",
    "                prepare_to_add.append(token)\n",
    "        if not skip_this_row:\n",
    "            for token in prepare_to_add:\n",
    "                maps[token].add(i)\n",
    "    print(f\"Abandon tokens:\", abandon_token)\n",
    "    return maps\n",
    "\n",
    "token_to_bznsyp_line = fit_dataset_to_token_set(bznsyp_ds, tokens_set)\n",
    "print(f\"len: {len(token_to_bznsyp_line)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_character(text: str):\n",
    "    return text.replace('，','').replace('。','').replace('—','').replace(\"“\",'').replace(\"”\",'').replace('？','').replace('！','').replace('：','').replace('！','').replace('、','').replace('；','').replace('…','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_length(dataset: typing.List[typing.Tuple[str, str]], fit_table: typing.Dict[str, typing.List[int]]):\n",
    "    ret: typing.Dict[str, typing.List[int]] = {}\n",
    "    for token, line_numbers in fit_table.items():\n",
    "        scores = [ 0 for _ in range(len(line_numbers)) ]\n",
    "        for i, line in enumerate(line_numbers):\n",
    "            text = dataset[line][0]\n",
    "            length = len(remove_non_character(text))\n",
    "            scores[i] = length\n",
    "        ret[token] = [ x[1] for x in sorted(zip(range(len(line_numbers)), line_numbers), key=lambda i: scores[i[0]], reverse=True) ]\n",
    "    return ret\n",
    "\n",
    "def sort_by_freq(dataset: typing.List[typing.Tuple[str, str]], fit_table: typing.Dict[str, typing.List[int]]):\n",
    "    ret: typing.Dict[str, typing.List[int]] = {}\n",
    "    for token, line_numbers in fit_table.items():\n",
    "        scores = [ 0 for _ in range(len(line_numbers)) ]\n",
    "        for i, line in enumerate(line_numbers):\n",
    "            for token in fix_bznsyp_tokens(dataset[line][1].split(' ')):\n",
    "                if token in fit_table:\n",
    "                    if len(fit_table[token]) == 1: # 稀有的token需要更容易选中\n",
    "                        scores[i] += 100\n",
    "                    elif len(fit_table[token]) == 2:\n",
    "                        scores[i] += 1\n",
    "        ret[token] = [ x[1] for x in sorted(zip(range(len(line_numbers)), line_numbers), key=lambda i: scores[i[0]], reverse=True) ]\n",
    "    return ret\n",
    "\n",
    "def select_first_n(fit_table: typing.Dict[str, typing.List[int]], n: int):\n",
    "    ret: typing.Dict[str, typing.List[int]] = {}\n",
    "    for token, line_numbers in fit_table.items():\n",
    "        for line in line_numbers[:n]:\n",
    "            if token not in ret:\n",
    "                ret[token] = [line]\n",
    "            else:\n",
    "                ret[token].append(line)\n",
    "    return ret\n",
    "\n",
    "def flatten_select_result(select_result: typing.Dict[str, typing.List[int]]):\n",
    "    select_sets: typing.Set[int] = set()\n",
    "    for key, value in select_result.items():\n",
    "        for v in value:\n",
    "            select_sets.add(v)\n",
    "    return select_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset token converage: 1231 compare to original 1301\n",
      "New dataset counts: 1038 compare to original 10000\n",
      "Data samples (length = 1038): [\n",
      "\t('进入秋季寡雨季节以来，泉州、漳州、莆田等沿海地区旱情明显。', 'jin4 ru4 qiu1 ji4 gua2 yu3 ji4 jie2 yi3 lai2 quan2 zhou1 zhang1 zhou1 pu2 tian2 deng3 yan2 hai3 di4 qu1 han4 qing2 ming2 xian3'),\n",
      "\t('在远处依稀可见的沙漠植物映衬下，整个球场显的绿意盎然。', 'zai4 yuan3 chu4 yi1 xi1 ke3 jian4 de5 sha1 mo4 zhi2 wu4 ying4 chen4 xia4 zheng3 ge5 qiu2 chang3 xian3 de5 lv4 yi4 ang4 ran2'),\n",
      "\t('由此，方体忠想到了姑奶张淑云，多次请其跟王文利“说说”。', 'you2 ci3 fang1 ti3 zhong1 xiang3 dao4 le5 gu1 nai3 zhang1 shu1 yun2 duo1 ci4 qing3 qi2 gen1 wang2 wen2 li4 shuo1 shuo5'),\n",
      "\t('一路上，孙颖浩用自己的方式鼓舞着队员，其实他的心里也忐忑不安。', 'yi2 lu4 shang4 sun1 ying3 hao4 yong4 zi4 ji3 de5 fang1 shi4 gu2 wu3 zhe5 dui4 yuan2 qi2 shi2 ta1 de5 xin1 li3 ye2 tan3 te4 bu4 an1'),\n",
      "\t('写散文的人最多，人心却像他们的文章一样散，闹也闲不出气势。', 'xie2 san3 wen2 de5 ren2 zui4 duo1 ren2 xin1 que4 xiang4 ta1 men5 de5 wen2 zhang1 yi2 yang4 san3 nao4 ye3 xian2 bu4 chu1 qi4 shi4'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "sorted_bznsyp_ds = { key: list(value) for key, value in token_to_bznsyp_line.items() }\n",
    "sorted_bznsyp_ds = sort_by_length(bznsyp_ds, sorted_bznsyp_ds)\n",
    "sorted_bznsyp_ds = select_first_n(sorted_bznsyp_ds, 6)\n",
    "sorted_bznsyp_ds = sort_by_freq(bznsyp_ds, sorted_bznsyp_ds)\n",
    "sorted_bznsyp_ds = select_first_n(sorted_bznsyp_ds, 3)\n",
    "\n",
    "selected_dataset = [ bznsyp_ds[i] for i in flatten_select_result(sorted_bznsyp_ds) ]\n",
    "subset_set = set(list(analysis_token_frequency(selected_dataset).keys()))\n",
    "print(f\"New dataset token converage: {len(tokens_set & subset_set)} compare to original {len(tokens_set & bznsyp_set)}\")\n",
    "print(f\"New dataset counts: {len(selected_dataset)} compare to original {len(bznsyp_ds)}\")\n",
    "print_dataset(selected_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_character_to_phomes(dataset: typing.List[typing.Tuple[str, str]]):\n",
    "    modified_ds = dataset.copy()\n",
    "    for k, (text, phome) in enumerate(dataset):\n",
    "        newphome = []\n",
    "        phomes = fix_bznsyp_tokens(phome.split(' '))\n",
    "        debug_phomes = phomes.copy()\n",
    "        skip_one = False\n",
    "        unhandle = False\n",
    "        for i, ch in enumerate(text):\n",
    "            if skip_one:\n",
    "                skip_one = False\n",
    "                continue\n",
    "            if ch in [\"。\", \"，\", \"—\", \"“\", \"”\", \"？\", \"！\", \"：\", \"、\", \"；\", \"…\"]:\n",
    "                newphome.append(ch)\n",
    "            else:\n",
    "                if len(phomes) == 0:\n",
    "                    unhandle = True\n",
    "                    break\n",
    "                p = phomes.pop(0)\n",
    "                newphome.append(p)\n",
    "        if len(phomes) == 0 and not unhandle:\n",
    "            modified_ds[k] = (text, ' '.join(newphome))\n",
    "        else:\n",
    "            print(f\"Unhandle: {k}\\n== {text}\\n== {debug_phomes}\\nremain: {phomes}\\n\")\n",
    "    return modified_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got final dataset.\n",
      "Data samples (length = 1038): [\n",
      "\t('进入秋季寡雨季节以来，泉州、漳州、莆田等沿海地区旱情明显。', 'jin4 ru4 qiu1 ji4 gua2 yu3 ji4 jie2 yi3 lai2 ， quan2 zhou1 、 zhang1 zhou1 、 pu2 tian2 deng3 yan2 hai3 di4 qu1 han4 qing2 ming2 xian3 。'),\n",
      "\t('在远处依稀可见的沙漠植物映衬下，整个球场显的绿意盎然。', 'zai4 yuan3 chu4 yi1 xi1 ke3 jian4 de sha1 mo4 zhi2 wu4 ying4 chen4 xia4 ， zheng3 ge qiu2 chang3 xian3 de lv4 yi4 ang4 ran2 。'),\n",
      "\t('由此，方体忠想到了姑奶张淑云，多次请其跟王文利“说说”。', 'you2 ci3 ， fang1 ti3 zhong1 xiang3 dao4 le gu1 nai3 zhang1 shu1 yun2 ， duo1 ci4 qing3 qi2 gen1 wang2 wen2 li4 “ shuo1 shuo ” 。'),\n",
      "\t('一路上，孙颖浩用自己的方式鼓舞着队员，其实他的心里也忐忑不安。', 'yi2 lu4 shang4 ， sun1 ying3 hao4 yong4 zi4 ji3 de fang1 shi4 gu2 wu3 zhe dui4 yuan2 ， qi2 shi2 ta1 de xin1 li3 ye2 tan3 te4 bu4 an1 。'),\n",
      "\t('写散文的人最多，人心却像他们的文章一样散，闹也闲不出气势。', 'xie2 san3 wen2 de ren2 zui4 duo1 ， ren2 xin1 que4 xiang4 ta1 men de wen2 zhang1 yi2 yang4 san3 ， nao4 ye3 xian2 bu4 chu1 qi4 shi4 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "cleaned_ds = add_extra_character_to_phomes(selected_dataset)\n",
    "print(f\"Got final dataset.\")\n",
    "print_dataset(cleaned_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_v2_dataset(filename: str, dataset: typing.List[typing.Tuple[str, str]]):\n",
    "\twith open(filename, 'w', encoding='utf-8') as f:\n",
    "\t\tfor (text, phome) in dataset:\n",
    "\t\t\tf.write(f\"{text}\\n{phome}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "save_v2_dataset(\"assets/text/mandarin_v2_train.txt\", cleaned_ds)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合v1数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_v2_dataset(filename: str):\n",
    "    dataset: typing.List[typing.Tuple[str, str]] = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(0, len(lines), 2):\n",
    "            text = lines[i].strip().strip('\\n')\n",
    "            phome = lines[i+1].strip().strip('\\n')\n",
    "            dataset.append((text, phome))\n",
    "    return dataset\n",
    "\n",
    "def parse_v1_dataset(filename: str):\n",
    "    dataset: typing.List[typing.Tuple[str, str]] = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            text = line.strip().strip('\\n')\n",
    "            if text == \"\": continue\n",
    "            cuts = list(jieba.cut(text))\n",
    "            phome = []\n",
    "            for cut in cuts:\n",
    "                pinyins = pypinyin.lazy_pinyin(cut, style=pypinyin.Style.TONE3, tone_sandhi=True)\n",
    "                for pinyin in pinyins:\n",
    "                    phome.append(pinyin)\n",
    "            phome = ' '.join(phome)\n",
    "            dataset.append((text, phome))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'assets/text/mandarin_v2_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m             dataset\u001b[38;5;241m.\u001b[39mappend((text, phome))\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m---> 11\u001b[0m cleaned_ds \u001b[38;5;241m=\u001b[39m \u001b[43mparse_v2_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massets/text/mandarin_v2_train.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m print_dataset(cleaned_ds)\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mparse_v2_dataset\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_v2_dataset\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m      2\u001b[0m     dataset: typing\u001b[38;5;241m.\u001b[39mList[typing\u001b[38;5;241m.\u001b[39mTuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m         lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(lines), \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\cosyvoice\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assets/text/mandarin_v2_train.txt'"
     ]
    }
   ],
   "source": [
    "cleaned_ds = parse_v2_dataset(\"assets/text/mandarin_v2_train.txt\")\n",
    "print_dataset(cleaned_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\CHENHA~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.423 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 729): [\n",
      "\t('他在家里吃苹果，喝茶，打电话，唱歌。', 'ta1 zai4 jia1 li3 chi1 ping2 guo3 ， he1 cha2 ， da3 dian4 hua4 ， chang4 ge1 。'),\n",
      "\t('我去商店买水果和书，看到小猫在玩。', 'wo3 qu4 shang1 dian4 mai3 shui3 guo3 he2 shu1 ， kan4 dao4 xiao3 mao1 zai4 wan2 。'),\n",
      "\t('小明去北京吃炸酱面，他在公园里看见了许多种类的鸟，甚至还遇到了一只大猩猩。', 'xiao3 ming2 qu4 bei3 jing1 chi1 zha2 jiang4 mian4 ， ta1 zai4 gong1 yuan2 li3 kan4 jian4 le xu3 duo1 zhong3 lei4 de niao3 ， shen4 zhi4 hai2 yu4 dao4 le yi4 zhi1 da4 xing1 xing1 。'),\n",
      "\t('小猫学会了跳舞，爬上了高高的树。', 'xiao3 mao1 xue2 hui4 le tiao4 wu3 ， pa2 shang4 le gao1 gao1 de shu4 。'),\n",
      "\t('风筝在天空中飞得又高又快，真是太漂亮了。', 'feng1 zheng1 zai4 tian1 kong1 zhong1 fei1 de2 you4 gao1 you4 kuai4 ， zhen1 shi4 tai4 piao4 liang4 le 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "v1_ds = parse_v1_dataset(\"assets/text/mandarin.txt\")\n",
    "print_dataset(v1_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token converage: 779\n"
     ]
    }
   ],
   "source": [
    "v1_ds_set = set(list(analysis_token_frequency(v1_ds).keys()))\n",
    "print(f\"Token converage: {len(tokens_set & v1_ds_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abandon tokens: {'AI', 'GDPR', '5G'}\n",
      "Abandon tokens: set()\n"
     ]
    }
   ],
   "source": [
    "fit_table_v1 = fit_dataset_to_token_set(v1_ds, tokens_set)\n",
    "fit_table_cleaned_ds = fit_dataset_to_token_set(cleaned_ds, tokens_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 646 tokens from v1 to v2.\n",
      "Tokens: ['pen', 'chun4', 'qiong1', 'bin', 'm', 'fo1', 'lei1', 'ri', 'nie', 'chun', 'o2', 'ce', 'eng', 'bi', 'juan3', 'lv1', 'nve1', 'rua', 'tu', 'nun4', 'dei4', 'lan1', 'kuan4', 'pian3', 'hang3', 'dun3', 'ruo1', 'xiu2', 'lin1', 'chuang', 'cuo', 'nve', 'n1', 'le2', 'lu1', 'neng4', 'gei', 'pou', 'kuo1', 'chua1', 'nong', 'se1', 'su3', 'gai2', 'ke', 'pie2', 'man', 'o3', 'pei3', 'jin', 'ran', 'ben', 'niang4', 'cun3', 'quan', 'xu', 'ping3', 'qiao3', 'niang1', 'nuan1', 'lin4', 'sun', 'lv', 'ping4', 'duan', 'pian', 'lia1', 'chuan', 'nou3', 'chi', 'bie', 'sao2', 'miu3', 'xiong4', 'miao', 'qiao', 'neng1', 'can', 'ruo2', 'chen1', 'cou3', 'hua3', 'jun2', 'nuo', 'diu2', 'shun2', 'chua4', 'xin2', 'cu', 'pin4', 'chen', 'yin', 'shai3', 'ca', 'shuang2', 'yue3', 'zen4', 'zen', 'ga2', 'tiao', 'chuo2', 'mu1', 'chuo3', 'piao', 'nin3', 'zhan', 'qia1', 'cen3', 'cao4', 'luan', 'cu3', 'cang', 'nuan4', 'heng3', 'jiong3', 'dun', 'jiang2', 'nang1', 'te1', 'kun', 'yao', 'nuan', 'nin4', 'duan2', 'rou1', 'ceng3', 'xin3', 'mou', 'ka', 'den3', 'na1', 'huang', 'sang', 'zhuan', 'mie', 'teng1', 'sun4', 'nou', 'fo', 'yue', 'zun', 'zhong', 'qiong3', 'zhou', 'kao1', 'bing', 'den', 'zhua2', 'zei3', 'keng', 'chua3', 'zan', 'rou3', 'biao', 'nian', 'fiao', 'li1', 'nu1', '.', 'o1', 'ning1', 'huai3', 'weng1', 'nou1', 'teng3', 'neng', 'mou1', 'run', 'lun', 'kuai1', 'zhui2', 'm3', 'dan', 'yun3', 'chuai2', 'keng3', 'pin', 'mao', 'zun2', 'reng4', 'san', 'tuan1', 'qiang', 'hao', 'de3', 'm2', 'fiao1', 'm1', 'nei1', 'beng2', 'nin', 'niao4', 'diu', 'dang2', 'tai', 'le3', 'cu2', 'zang2', 'sui', 'fou4', 'pu', 'she', 'chai3', 'nai1', 'ling1', 'chong', 'eng4', 'suan3', 'chai', 'seng2', 'ru1', 'wai3', 'neng3', 'er1', 'hun', 'kuo3', 'zhui3', 'rong', 'kao', 'nin1', 'rui1', 'liao1', 'dang', 'chui4', 'dia4', 'gao', 'qian3', 'hou1', 'nen1', 'jiong4', 'guai', 'a2', 'rui', 'shan', 'wai2', 'mie3', 'hang', 'wang', 'le1', 'reng3', 'ze1', 'lo1', 'sao4', 'lo3', 'xun3', 'zhuo3', 'jue', 'n2', 'lve1', 'tai3', 'shua', 'bei', 'liang1', 'ruan1', 'seng', 'kuai2', 'ruan', 'seng4', 'rong4', 'ze3', 'zui', 'ei2', 'qun4', 'gai', 'zou', 'nan', 'hang1', 'pou3', 'gui', 'jue3', 'ri2', 'gun1', 'nen3', 'min1', 'dia2', 'bie3', '0', '\"', 'cuan3', 'jiang', 'cheng', 'hei', 'chao4', 'cheng4', 'tuan4', 'jiong1', 'zei4', 'niang3', 'n', 'ga', 'lie3', 'gen3', 'kang', 'ruo', 'pao', 'mang4', 'ao', 'an', 'die4', 'me4', 'dei1', 'keng4', 'ci', 'se3', 'la3', 'eng3', 'zei1', 'shui', 'en', 'tian', 'mie2', 'ken4', 'en2', 'zhang', 're1', 'xuan', 'min4', 'chuai', 'shou', 'zeng3', 'huang4', 'xun', ',', 'pang', 'juan', 'nou2', 'guai2', 'wei', 'huai1', 'ran4', 'dui2', 'en3', 'zhuai2', 'lve3', 'pie', 'min', 'cou1', 'wai', 'wo', ':', 'ou4', 'tie', 'luan1', 'che', 'geng', 'nan1', 'rao', 'jun', 'qia2', 'yo4', 'hun3', 'cong3', 'zhen', 'lo2', 'mei1', 'ze4', 'nuo3', 'dei', 'fiao4', 'fiao2', 'nve2', 'sang2', 'que', 'kuan', 'jun3', 'dia', 'xue', 'dai2', 'zhou2', 'zui1', 'nv', 'leng1', 'nie3', 'cuo3', 'fou', 'hm', 'beng3', \"'\", 'gen4', 'ku', 'teng4', 'nun2', 'ha3', 'ding', 'dia3', 'te2', 'zhua4', 'chuang3', 'se2', 'chuo', 'lian1', 're', 'pou2', 'zhai', '?', 'zun4', 'ei1', 'ca4', 'ken', 'fei', 'hm3', 'qiu4', 'zeng', 'o4', 'ka4', 'qiong', 'shuang', 'yo3', 'mang1', 'die3', 'ruan4', 'miu1', 'zhuo4', 'gen2', 'ren1', 'zhuo', 'ne3', 'weng', 'pen3', 'qun3', 'ceng', 'nun1', 'nen2', 'chai4', 'hei2', 'nie2', 'zen1', 'cen', 'he3', 'ming1', 'cang4', 'a3', 'm4', 'qun', 'shai2', 'guan', 'sou2', 'tun4', 'ne1', 'ran1', 'yu', 'ceng1', 'kang3', 'niao', 'xia3', 'fiao3', 'shuang4', 'rua1', 'gei4', 'cou', 'keng2', 'song', 'dui3', '_', 'tian4', 'shun1', 'xiang', 'den1', 'ou', 'kui', 'ruo3', 'ning', 'dia1', 'sai3', 'lia4', 'huai', 'qiong4', 'bin3', 'kua', 'nun', 'kuan2', 'fan', 'den4', 'hei3', 'kuo2', 'zhuai3', 'cuan2', 'dong', 'men3', 'miu2', 'mou4', 'ze', 'ta', 'hm2', 'pie4', 'niao1', 'kai', 'nv1', 'me2', 'ca2', 'pou4', 'shuo3', 'nou4', 'miu', 'shuan2', 'dai', 'ri3', 'mei', 'mian1', 'rou', 'cen4', 'fou1', 'hua', 'ni', 'beng', 'lun3', 'shuan3', 'zhuang3', 'zuan', 'ce2', 'ce3', 'an2', 'kang2', 'ang', 'hei4', 'ang2', 'suo4', 'zun3', 'den2', 'nang4', 'me3', 'zu4', 'hm1', 'kuo', 'pa3', 'yue2', 'rua4', 'long1', 'cuan4', 'chui3', 'dun2', 'rao1', 'shui1', 'pen2', 'luo1', 'que3', 'ang3', 'cou2', 'nei2', 'de1', 'ei4', 'ei3', 'pei', 'tou3', 'zhuai', 'man1', 'hen', 'chua', 'ha4', 'shua4', 'kuang3', 'ce1', 'lve2', 'cun', 'seng3', 'nv4', 'eng2', 'mai1', 'ting4', 'ru', 'yo2', 'chan4', 'meng', 'cong4', 'te3', 'zeng2', 'nun3', 'cuan', 'kuai3', 'jia2', 'gei1', 'rong1', 'fo4', 'xia', 'hm4', 'za', 'leng', 'suan2', 'sen4', 'shuo2', 'zang', 'zei', 'zhuai1', 'rua2', 'nuo1', 'sai2', 'nu', 'lve', 'sen3', 'bu1', 'ken1', 'tun3', 'chu', 'nong1', 'diu3', 'sou3', 'zuo1', '!', 'pai3', 'pan3', 'lie1', 'mang', 'reng', 'eng1', 'nve3', 'xian', 'yun', 'fo3', 'n4', 'n3', 'shun', 'lai1', 'zhui', 'zhua', 'lai3', 'nang3', 'zhao', 'xiu', 'run1', 'sen', 'hen1', 'ri1', 'za4', 'run3', 'hong', 'jiong', 'qia', 'diu4', 'zhun4', 'zang3', 'deng', 'kong', 'xiong3', 'rua3', 'nei', 'lo4', 'pang3', 'cang3', 'zhuang2', 'nong3', 'chua2', 'lei', 'diao2', 'run2', 'sen2', 'bin2']\n"
     ]
    }
   ],
   "source": [
    "append_v1_to_v2_tokens: typing.List[str] = []\n",
    "for token, lines in fit_table_v1.items():\n",
    "    if len(fit_table_cleaned_ds[token]) == 0:\n",
    "        append_v1_to_v2_tokens.append(token)\n",
    "print(f\"Merge {len(append_v1_to_v2_tokens)} tokens from v1 to v2.\")\n",
    "print(f\"Tokens: {append_v1_to_v2_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 2): [\n",
      "\t('老师带我们参观了博物馆，讲解了许多古老的文物。', 'lao3 shi1 dai4 wo3 men can1 guan1 le bo2 wu4 guan3 ， jiang2 jie3 le xu3 duo1 gu3 lao3 de wen2 wu4 。'),\n",
      "\t('老师讲解了很多难题，大家都明白了。', 'lao3 shi1 jiang2 jie3 le hen3 duo1 nan2 ti2 ， da4 jia1 dou1 ming2 bai2 le 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "sub_fit_table = { key: list(value) for key, value in fit_table_v1.items() if key in append_v1_to_v2_tokens }\n",
    "sub_fit_table = sort_by_freq(v1_ds, sub_fit_table)\n",
    "sub_fit_table = select_first_n(sub_fit_table, 3)\n",
    "sub_v1_ds = [ v1_ds[i] for i in flatten_select_result(sub_fit_table) ]\n",
    "print_dataset(sub_v1_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_v2_dataset(\"assets/text/mandarin_v2_noletters.txt\", sub_v1_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合cross letters数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 21): [\n",
      "\t('从A到B的计划需要C和D的配合，以确保E可以成功执行。', 'cong2 A dao4 B de ji4 hua4 xu1 yao4 C he2 D de pei4 he2 ， yi3 que4 bao3 E ke2 yi3 cheng2 gong1 zhi2 xing2 。'),\n",
      "\t('你从F到G的过程中，H的支持是不可缺少的，尤其是在I阶段。', 'ni3 cong2 F dao4 G de guo4 cheng2 zhong1 ， H de zhi1 chi2 shi4 bu4 ke3 que1 shao3 de ， you2 qi2 shi4 zai4 I jie1 duan4 。'),\n",
      "\t('在项目进行中，J和K的合作将直接影响C的效率。', 'zai4 xiang4 mu4 jin4 xing2 zhong1 ， J he2 K de he2 zuo4 jiang1 zhi2 jie1 ying3 xiang3 C de xiao4 lv4 。'),\n",
      "\t('如果你想提高L到M的速度，N的调整将非常重要。', 'ru2 guo3 ni3 xiang3 ti2 gao1 L dao4 M de su4 du4 ， N de tiao2 zheng3 jiang1 fei1 chang2 zhong4 yao4 。'),\n",
      "\t('在这次活动中，O的出现为P带来了更大的机会。', 'zai4 zhe4 ci4 huo2 dong4 zhong1 ， O de chu1 xian4 wei4 P dai4 lai2 le geng4 da4 de ji1 hui4 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "crossletters_ds = parse_v1_dataset(\"assets/text/crossletters.txt\")\n",
    "print_dataset(crossletters_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 21): [\n",
      "\t('从A到B的计划需要C和D的配合，以确保E可以成功执行。', 'cong2 letterA dao4 letterB de ji4 hua4 xu1 yao4 letterC he2 letterD de pei4 he2 ， yi3 que4 bao3 letterE ke2 yi3 cheng2 gong1 zhi2 xing2 。'),\n",
      "\t('你从F到G的过程中，H的支持是不可缺少的，尤其是在I阶段。', 'ni3 cong2 letterF dao4 letterG de guo4 cheng2 zhong1 ， letterH de zhi1 chi2 shi4 bu4 ke3 que1 shao3 de ， you2 qi2 shi4 zai4 letterI jie1 duan4 。'),\n",
      "\t('在项目进行中，J和K的合作将直接影响C的效率。', 'zai4 xiang4 mu4 jin4 xing2 zhong1 ， letterJ he2 letterK de he2 zuo4 jiang1 zhi2 jie1 ying3 xiang3 letterC de xiao4 lv4 。'),\n",
      "\t('如果你想提高L到M的速度，N的调整将非常重要。', 'ru2 guo3 ni3 xiang3 ti2 gao1 letterL dao4 letterM de su4 du4 ， letterN de tiao2 zheng3 jiang1 fei1 chang2 zhong4 yao4 。'),\n",
      "\t('在这次活动中，O的出现为P带来了更大的机会。', 'zai4 zhe4 ci4 huo2 dong4 zhong1 ， letterO de chu1 xian4 wei4 letterP dai4 lai2 le geng4 da4 de ji1 hui4 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(crossletters_ds)):\n",
    "    text = crossletters_ds[i][0]\n",
    "    phome = crossletters_ds[i][1]\n",
    "    newphome = []\n",
    "    for x in phome.split(' '):\n",
    "        if len(x) == 1 and ord('A') <= ord(x[0]) and ord(x[0]) <= ord('Z'):\n",
    "            newphome.append(f'letter' + x)\n",
    "        else:\n",
    "            newphome.append(x)\n",
    "    crossletters_ds[i] = (text, ' '.join(newphome))\n",
    "print_dataset(crossletters_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合后再压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 1061): [\n",
      "\t('进入秋季寡雨季节以来，泉州、漳州、莆田等沿海地区旱情明显。', 'jin4 ru4 qiu1 ji4 gua2 yu3 ji4 jie2 yi3 lai2 ， quan2 zhou1 、 zhang1 zhou1 、 pu2 tian2 deng3 yan2 hai3 di4 qu1 han4 qing2 ming2 xian3 。'),\n",
      "\t('在远处依稀可见的沙漠植物映衬下，整个球场显的绿意盎然。', 'zai4 yuan3 chu4 yi1 xi1 ke3 jian4 de sha1 mo4 zhi2 wu4 ying4 chen4 xia4 ， zheng3 ge qiu2 chang3 xian3 de lv4 yi4 ang4 ran2 。'),\n",
      "\t('由此，方体忠想到了姑奶张淑云，多次请其跟王文利“说说”。', 'you2 ci3 ， fang1 ti3 zhong1 xiang3 dao4 le gu1 nai3 zhang1 shu1 yun2 ， duo1 ci4 qing3 qi2 gen1 wang2 wen2 li4 “ shuo1 shuo ” 。'),\n",
      "\t('一路上，孙颖浩用自己的方式鼓舞着队员，其实他的心里也忐忑不安。', 'yi2 lu4 shang4 ， sun1 ying3 hao4 yong4 zi4 ji3 de fang1 shi4 gu2 wu3 zhe dui4 yuan2 ， qi2 shi2 ta1 de xin1 li3 ye2 tan3 te4 bu4 an1 。'),\n",
      "\t('写散文的人最多，人心却像他们的文章一样散，闹也闲不出气势。', 'xie2 san3 wen2 de ren2 zui4 duo1 ， ren2 xin1 que4 xiang4 ta1 men de wen2 zhang1 yi2 yang4 san3 ， nao4 ye3 xian2 bu4 chu1 qi4 shi4 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "total_ds: typing.List[typing.Tuple[str, str]] = []\n",
    "total_ds.extend(cleaned_ds)\n",
    "total_ds.extend(sub_v1_ds)\n",
    "total_ds.extend(crossletters_ds)\n",
    "print_dataset(total_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token counts: 2095\n"
     ]
    }
   ],
   "source": [
    "new_tokens_set = tokens_set.copy()\n",
    "for i in range(ord('A'), ord('Z') + 1):\n",
    "    new_tokens_set.add(f'letter' + chr(i)) # crossletters中新增的tokens\n",
    "print(f\"Token counts: {len(new_tokens_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abandon tokens: set()\n",
      "Data samples (length = 596): [\n",
      "\t('进入秋季寡雨季节以来，泉州、漳州、莆田等沿海地区旱情明显。', 'jin4 ru4 qiu1 ji4 gua2 yu3 ji4 jie2 yi3 lai2 ， quan2 zhou1 、 zhang1 zhou1 、 pu2 tian2 deng3 yan2 hai3 di4 qu1 han4 qing2 ming2 xian3 。'),\n",
      "\t('在远处依稀可见的沙漠植物映衬下，整个球场显的绿意盎然。', 'zai4 yuan3 chu4 yi1 xi1 ke3 jian4 de sha1 mo4 zhi2 wu4 ying4 chen4 xia4 ， zheng3 ge qiu2 chang3 xian3 de lv4 yi4 ang4 ran2 。'),\n",
      "\t('同时，义工们也正在筹备经费购买御寒衣物派送给孤寡老人。', 'tong2 shi2 ， yi4 gong1 men ye3 zheng4 zai4 chou2 bei4 jing1 fei4 gou4 mai3 yu4 han2 yi1 wu4 pai4 song4 gei3 gu1 gua2 lao3 ren2 。'),\n",
      "\t('但如果按车队规模，一嗨数千辆车的量级绝对算不上最大。', 'dan4 ru2 guo3 an4 che1 dui4 gui1 mo2 ， yi4 hai1 shu4 qian1 liang4 che1 de liang4 ji2 jue2 dui4 suan4 bu2 shang4 zui4 da4 。'),\n",
      "\t('回到家林妈不住催问，他说还可以，林母拍腿而起：“你说可以就是不好！', 'hui2 dao4 jia1 lin2 ma1 bu2 zhu4 cui1 wen4 ， ta1 shuo1 hai2 ke2 yi3 ， lin2 mu3 pai1 tui3 er2 qi3 ： “ ni3 shuo1 ke2 yi3 jiu4 shi4 bu4 hao3 ！'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "fit_table_final = fit_dataset_to_token_set(total_ds, new_tokens_set)\n",
    "clean2_ds = { key: list(value) for key, value in fit_table_final.items() }\n",
    "clean2_ds = sort_by_freq(total_ds, clean2_ds)\n",
    "clean2_ds = select_first_n(clean2_ds, 3)\n",
    "final_ds = [ total_ds[i] for i in flatten_select_result(clean2_ds) ]\n",
    "print_dataset(final_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token converage: 1407\n"
     ]
    }
   ],
   "source": [
    "final_ds_set = set(list(analysis_token_frequency(final_ds).keys()))\n",
    "print(f\"Token converage: {len(new_tokens_set & final_ds_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_v2_dataset(\"assets/text/mandarin_v2_train_withletters.txt\", final_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制作验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ds = parse_v2_dataset(\"assets/text/mandarin_v2_train_withletters.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selective range: 9423\n"
     ]
    }
   ],
   "source": [
    "train_texts: typing.Set[str] = set()\n",
    "for (text, phome) in final_ds:\n",
    "    train_texts.add(text)\n",
    "select_range: typing.List[int] = []\n",
    "for i, (text, phome) in enumerate(bznsyp_ds):\n",
    "    if text not in train_texts:\n",
    "        select_range.append(i)\n",
    "print(f\"Selective range: {len(select_range)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples (length = 30): [\n",
      "\t('今年在外包装上还首次有了防伪记号。', 'jin1 nian2 zai4 wai4 bao1 zhuang1 shang4 hai2 shou3 ci4 you3 le fang2 wei3 ji4 hao4 。'),\n",
      "\t('既想让马儿跑，又不给马儿足够的草，其结果也就在意料之中。', 'ji4 xiang3 rang4 ma3 er2 pao3 ， you4 bu4 gei2 ma3 er2 zu2 gou4 de cao3 ， qi2 jie2 guo2 ye3 jiu4 zai4 yi4 liao4 zhi1 zhong1 。'),\n",
      "\t('最后，张金中在漆黑的夜色下涮洗。', 'zui4 hou4 ， zhang1 jin1 zhong1 zai4 qi1 hei1 de ye4 se4 xia4 shuan4 xi3 。'),\n",
      "\t('赵云的朋友郝超英建议先带董静去海口躲躲。', 'zhao4 yun2 de peng2 you hao3 chao1 ying1 jian4 yi4 xian1 dai4 dong3 jing4 qu4 hai2 kou3 duo2 duo3 。'),\n",
      "\t('救援队正在加快安装固定泵，通过抽水稳定水位。', 'jiu4 yuan2 dui4 zheng4 zai4 jia1 kuai4 an1 zhuang1 gu4 ding4 beng4 ， tong1 guo4 chou1 shui2 wen3 ding4 shui3 wei4 。'),\n",
      ", ...]\n"
     ]
    }
   ],
   "source": [
    "select_idx = random.sample(select_range, 30)\n",
    "valid_ds = [ bznsyp_ds[i] for i in select_idx ]\n",
    "valid_ds = add_extra_character_to_phomes(valid_ds)\n",
    "print_dataset(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_v2_dataset(\"assets/text/mandarin_v2_valid.txt\", valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成tokens文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens saved to assets/text/mandarin_v2_train_withletters_tokens.txt\n"
     ]
    }
   ],
   "source": [
    "from utils.tokens import generate_token_list\n",
    "token_filename = os.path.join(\"assets/text/mandarin_v2_train_withletters_tokens.txt\")\n",
    "with open(token_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "\ttokens = generate_token_list()\n",
    "\tfor indx, token in enumerate(tokens):\n",
    "\t\tf.write(f\"{token} {indx}\\n\")\n",
    "\tcnt = len(tokens)\n",
    "\tfor i in range(ord('A'), ord('Z') + 1):\n",
    "\t\tf.write(f'letter{chr(i)} {cnt}\\n')\n",
    "\t\tcnt += 1\n",
    "print(f\"Tokens saved to {token_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
